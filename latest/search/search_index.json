{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"Robust Equilibration Detection <p>A Python package for detecting equilibration in timeseries data where an initial transient is followed by a stationary distribution. Two main approaches are implemented, which differ in the way they account for autocorrelation:</p> <ul> <li><code>detect_equilibration_init_seq</code>: This uses the initial sequence methods of Geyer (Geyer, 1992) to determine the truncation point of the sum of autocovariances. Chodera's method (Chodera, 2016) of simply truncating the autocovariance series at the first negative value is also implemented.</li> <li><code>detect_equilibration_window</code>: This uses window methods (see Geyer again) when calculating the autocorrelation. Setting the window size to 1 will give you White's original Marginal Standard Error Rule (White, 1997).</li> </ul> <p>For both, the equilibration point can be determined either according to the minimum of the squared standard error (the default), or the maximum effective sample size, by specifying <code>method=\"min_sse\"</code> or <code>method=\"max_ess\"</code>.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/fjclark/red.git\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Warning: <code>red</code> will work with multi-run data, but has only been thoroughly tested with single-run data. Using multi-run data is likely to be more robust, but we have not verified this.</p>"},{"location":"#equilibration-detection","title":"Equilibration Detection","text":"<pre><code>import red\n\n# Load your timeseries of interest.\n# This should be a 2D numpy array with shape (n_runs, n_samples),\n# or a 1D numpy array with shape (n_samples).\nmy_timeseries = ...\n\n# Detect equilibration based on the minimum squared standard error using\n# using the window method with a Bartlett kernel with a window size of\n# round(n_samples**0.5) to account for autocorrelation. idx is the index\n# of the first sample after equilibration, g is the statistical\n# inefficiency of the equilibrated sample, and ess is the effective sample\n# size of the equilibrated sample.\nidx, g, ess = red.detect_equilibration_window(my_timeseries,\n                                              method=\"min_sse\",\n                                              plot=True)\n\n# Alternatively, use Geyer's initial convex sequence method to account\n# for autocorrelation.\nidx, g, ess = red.detect_equilibration_init_seq(my_timeseries,\n                                                method=\"min_sse\",\n                                                plot=True)\n\n# We can also determine equilibration in the same way as in\n# pymbar.timeseries.detect_equilibration(my_timeseries, fast=False)\nidx, g, ess = red.detect_equilibration_init_seq(my_timeseries,\n                                                method=\"max_ess\",\n                                                sequence_estimator=\"positive\")\n</code></pre>"},{"location":"#uncertainty-quantification","title":"Uncertainty Quantification","text":"<pre><code># Estimate the 95 % confidence interval, accounting for autocorrelation using Geyer's initial\n# convex sequence method.\nci_95 = red.get_conf_int_init_seq(my_timeseries, alpha_two_tailed=0.05)\n</code></pre> <p>For more examples, see the documentation.</p>"},{"location":"#copyright","title":"Copyright","text":"<p>Copyright (c) 2023, Finlay Clark</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Project based on the Computational Molecular Science Python Cookiecutter version 1.1, with several ideas (Makefile, documentation) borrowed from Simon Boothroyd's super helpful python-template.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#010-2024-09-23","title":"[0.1.0] - 2024-09-23","text":"<p>Initial release.</p>"},{"location":"development/","title":"Development","text":"<p>To create a development environment, you must have <code>mamba</code> installed.</p> <p>A development conda environment can be created and activated with:</p> <pre><code>make env\nmamba activate red\n</code></pre> <p>Some handy <code>make</code> commands are available: <pre><code>make lint # Lint the codebase with Ruff\nmake format # Format the codebase with Ruff\nmake type-check # Type-check the codebase with Mypy\nmake test # Run the unit tests with Pytest\n</code></pre></p> <p>To serve the documentation locally:</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"examples/","title":"Examples","text":"Recommended methods <p>For equilibration detection, we recommend starting with the <code>detect_equilibration_window</code> function with \"min_sse\" method and a window size function of <code>round(n_samples**0.5)</code> (the defaults): <pre><code>idx, g, ess = red.detect_equilibration_window(my_timeseries, plot=True)\nmy_truncated_timeseries = my_timeseries[idx:]\n</code></pre></p> <p>For uncertainty_estimation, we recommend using Geyer's initial convex sequence method (Geyer, 1992), which is the default for <code>get_variance_initial_sequence</code>: <pre><code>var, max_lag, acovf = red.get_variance_initial_sequence(my_timeseries)\nstd_err = np.sqrt(var)\n</code></pre></p> <p>This package provides a range of methods for selecting the truncation point of a time series, where the aim is to remove an initial \"unequilibrated\" portion of the data. Functionality is also provided for estimating the variance of the mean of a time series, which is useful for uncertainty estimation. For details, please see the theory page.</p> <p>For all examples, <code>my_timeseries</code> should be a numpy array with shape <code>(n_samples)</code>, or <code>(n_repeats, n_samples)</code> if you have multiple repeats of the same simulation.</p> Warning <p>These methods only been thoroughtly tested on-single run data. Using multi-run data is likely to be more robust, but we have not verified this.</p>"},{"location":"examples/#detecting-equilibration","title":"Detecting Equilibration","text":""},{"location":"examples/#initial-sequence-methods","title":"Initial Sequence Methods","text":"<p>To use any of Geyer's initial sequence methods (Geyer, 1992), you can specify the \"sequence_estimator\" to be \"initial_positive\" (the least strict), \"initial_monotone\", or \"initial_convex\" (the strictest):</p> <p><pre><code>idx, g, ess = red.detect_equilibration_init_seq(my_timeseries, sequence_estimator=\"initial_convex\", plot=True)\nmy_truncated_timeseries = my_timeseries[idx:]\n</code></pre> To use Chodera's method of simply truncating the autocovariance series at the first negative value (Chodera, 2016), you can specify the \"sequence estimator\" to be \"positive\".</p>"},{"location":"examples/#window-methods","title":"Window Methods","text":"<p>When using window methods, you can either specify a fixed window size, or a window size function which computes the window size as a function of the number of data points (which decreases as the truncation point increases). These are specified via <code>window_size</code> and <code>window_size_fn</code>, respectively (one must be specified and the other must be <code>None</code>). The default window size function is <code>lambda x: round(x**0.5)</code> - explicitly:</p> <pre><code>idx, g, ess = red.detect_equilibration_window(my_timeseries, window_size=None, window_size_fn=lambda x: round(x**0.5), plot=True)\n# This is equivalent to:\nidx, g, ess = red.detect_equilibration_window(my_timeseries, plot=True)\n</code></pre> <p>To use a window size of 10:</p> <pre><code>idx, g, ess = red.detect_equilibration_window(my_timeseries, window_size=10, window_size_fn = None, plot=True)\n</code></pre> <p>You can also play with the kernel function used in the window method by specifying the <code>kernel</code> argument. You should supply the function directly - the default is <code>np.bartlett</code>.</p>"},{"location":"examples/#the-original-marginal-standard-error-rule","title":"The Original Marginal Standard Error Rule","text":"<p>To use White's original Marginal Standard Error Rule (White, 1997), you can use the window method with a window size of 1:</p> <pre><code>idx, g, ess = red.detect_equilibration_window(my_timeseries, window_size=1, window_size_fn=None, plot=True)\n</code></pre>"},{"location":"examples/#maximum-effective-sample-size-and-choderas-method","title":"Maximum Effective Sample Size and Chodera's Method","text":"<p>To select the truncation point according to the maximum effective sample size (instead of the minimum squared standard error), you can specify the <code>method</code> argument to be \"max_ess\". To use Chodera's method (Chodera, 2016) as implemented in <code>pymbar.timeseries</code>, you can specify the <code>sequence_estimator</code> to be \"positive\":</p> <pre><code>idx, g, ess = red.detect_equilibration_init_seq(my_timeseries, method=\"max_ess\", sequence_estimator=\"positive\", plot=True)\n# Equivalent to pymbar.timeseries.detect_equilibration(my_timeseries, fast=False)\n</code></pre>"},{"location":"examples/#plotting","title":"Plotting","text":"<p>To save a plot showing the (block-averaged) time series and variance of the mean/ effective sample size against truncation time, simply specify <code>plot=True</code> and, optionally, specify a name for the plot with <code>plot_name</code>. This works for either of the equilbration detection functions.</p> <pre><code>idx, g, ess = red.detect_equilibration_window(my_timeseries, plot=True, plot_name=\"my_equilibration_plot.png\")\n</code></pre>"},{"location":"examples/#estimating-uncertainty","title":"Estimating Uncertainty","text":"<p>To calculate uncertainty, we recommend using Geyer's initial convex sequence method (Geyer, 1992), which is the default for <code>get_conf_int_init_seq</code>. For example, to estimate a 95 % confidence interval:</p> <pre><code>ci_95 = red.get_conf_int_init_seq(my_timeseries, sequence_estimator=\"initial_convex\", alpha_two_tailed=0.05)\n</code></pre> <p>This function has a similar interface to <code>detect_equilibration_init_seq</code>, and you can specify the \"sequence_estimator\" in the same way. Note that this assumes we have a reasonable effective sample size, and hence that the means are approximately normally distributed by the central limit theorem.</p> Warning <p>Estimates of uncertainty based on single runs are very likely to be underestimates. Estimating uncertainty from deviations between repeat runs is more robust.</p>"},{"location":"theory/","title":"Theory","text":"Info <p>To see examples of these methods in action, see the examples page.</p>"},{"location":"theory/#equilibration-detection","title":"Equilibration Detection","text":"<p>This package provides a range of methods for selecting the truncation point of a time series, where the aim is to remove an initial \"unequilibrated\" portion of the data. These methods differ in the way they account for autocorrelation in the data, and whether they select the truncation point based on the minimum of the squared standard error or the maximum effective sample size. When using the minimum of the squared standard error, we can think of these as generalisations of the Marginal Standard Error Rule (MSER, White, 1997). Formally, the generalised MSER is:</p> \\[\\begin{align}     n_0* &amp;= \\operatorname*{arg min}_{N &gt; n_0 &gt; 0}\\left[\\widehat{\\mathrm{Var}}(\\langle A \\rangle_{[n_{0},N]})\\right],      \\label{eqn:mser_correlated} \\end{align}\\] <p>which means that we select the \"optimal\" truncation point, \\(n_0*\\), so that it minimises \\(\\widehat{\\mathrm{Var}}(\\langle A \\rangle_{[n_{0},N]})\\). \\(\\widehat{\\mathrm{Var}}(\\langle A \\rangle_{[n_{0},N]})\\) is the estimated variance of the mean of some observable \\(A(\\mathbf{x})\\) over the range \\([n_{0},N]\\), where \\(n_0\\) is the number of your first data point, and \\(N\\) is the number of your final data point. The naive estimator of the variance of the mean is:</p> \\[\\begin{align}     \\widehat{\\mathrm{Var}}_{\\mathrm{Naive}}(\\langle A \\rangle_{[n_{0},N]}) &amp;= \\frac{1}{N_{n_0}}\\sum^{N_{n_0}-1}_{t=-(N_{n_0}-1)}\\hat{\\gamma}_{t,[n_{0},N]}     \\label{eqn:var_naive_unsplit}\\\\     &amp;= \\frac{1}{N_{n_0}}\\left(\\hat{\\gamma}_{0,[n_{0},N]} +  2\\sum^{N_{n_0}-1}_{t=1}\\hat{\\gamma}_{t,[n_{0},N]}\\right),     \\label{eqn:var_naive} \\end{align}\\] <p>where \\(N_{n_0} = N - n_0 + 1\\) and the autocovariance terms are estimated as:</p> \\[\\begin{align}     \\hat{\\gamma}_{t,[n_0, N]} &amp;= \\frac{1}{N_{n_0}} \\sum^{N-t}_{n=n_0} (A(\\mathbf{x}_n) - \\langle A \\rangle_{[n_{0},N]})(A(\\mathbf{x}_{n+t}) - \\langle A \\rangle_{[n_{0},N]}).     \\label{eqn:autocov_estimate} \\end{align}\\] <p>The key issue is that we can't add up all the autocovariance terms, as our estimate of the variance would become very noisy. The methods discussed calculate this sum differently. We'll start with the methods which most rigorously account for correlation by including the most terms, then move through the spectrum of methods to end with the original MSER, which does not account for correlation at all and only includes the \\(\\hat{\\gamma}_{0,[n_0, N]}\\) term.</p>"},{"location":"theory/#initial-sequence-methods-fully-account-for-autocorrelation","title":"Initial sequence methods fully account for autocorrelation","text":"<p>Geyer's initial sequence methods (Geyer, 1992) apply certain rules to the sum of autocovariance terms to ensure that they make sense of Markov chains. The initial sequence methods are the most rigorous in terms of accounting for autocorrelation, in that they include the most terms from the autocovariance sum. Geyer's methods, in order of increasing strictness, are \"initial positive\" &lt; \"initial monotone\" &lt; \"Initial convex\". Chodera proposed simply truncating the sum at the first negative value (Chodera, 2016) - we include this method with the initial sequence methods. These are implemented in the <code>detect_equilibration_init_seq</code> function.</p>"},{"location":"theory/#the-window-method-partially-accounts-for-autocorrelation","title":"The window method partially accounts for autocorrelation","text":"<p>The window method (Geyer, 1992) weights each of the terms in the autocovariance sum according to some \"window\" function. This method includes more or less terms of the autocovariance series depending on the window size. The window can be of fixed size, or change with \\(N_{n_0}\\). These are implemented in the <code>detect_equilibration_window</code> function.</p>"},{"location":"theory/#mser-the-method-which-ignores-autocorrelation","title":"MSER: The method which ignores autocorrelation","text":"<p>White's original Marginal Standard Error Rule (White, 1997) simply truncates the autocovariance sum at the first term, \\(\\hat{\\gamma}_{0,[n_0, N]}\\), so that autocorrelation is ignored. You can think of it as a special case of the window method with a window size of 1.</p>"},{"location":"theory/#ess","title":"ESS","text":"<p>Selecting the truncation point based on the maximum effective sample size amounts to using the formula:</p> \\[\\begin{align}     n_0* &amp;= \\operatorname*{arg min}_{N &gt; n_0 &gt; 0}\\left[\\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{Trajs}}(\\langle A \\rangle_{[n_{0},N]})}{\\widehat{\\mathrm{Var}}_{[n_{0},N]}(A(\\mathrm{\\mathbf{x}}))}  \\right]     \\label{eqn:max_ess_algo} \\end{align}\\] <p>which is very similar to the generalised MSER formula. We've found that we generally get very similar results using each approach, but that the min SSE approach sometimes avoids issues with one or two very different samples at the start of the timeseries (where the ESS approach selects a truncation point that includes these samples).</p>"},{"location":"theory/#which-method-should-i-use","title":"Which method should I use?","text":"Recommended method <p>For equilibration detection we recommend starting with the window method with window size \\(\\sqrt{N_{n_0}}\\), which is the default for <code>detect_equilibration_window</code>.</p> <p>We've tested the above generalised MSER methods on synthetic single-run data modelled on absolute binding free energy calculations for a range of systems. In general, the methods which more thoroughly accounted for autocorrelation we more liable to choose late and variable truncation points, while the methods which less thoroughly accounted for autocorrelation were liable to truncate early. We found that the original MSER was generally the worst-performing method, and that the window method with a window size of \\(\\sqrt{N_{n_0}}\\) was the best-performing method, striking a balance between accounting for autocorrelation and not truncating too late/ variably. However, the best method for your data may vary, so we recommend trying a few different methods on a few test time series to get a feel for the best method for you.</p>"},{"location":"theory/#uncertainty-estimation","title":"Uncertainty Estimation","text":"Recommended method <p>For uncertainty estimation we recommend using Geyer's initial convex sequence method, which is the default for <code>get_conf_int_init_seq</code>.</p> <p>Estimating the uncertainty in the mean of a time series is a different problem to finding the optimum truncation point. In this case, you want to rigorously account for autocorrelation to avoid underestimating the uncertainty. Therefore, we recommend Geyer's initial convex sequence method (Geyer, 1992) as the strictest of the initial sequence methods. Note that selecting your equilibration point based on the minimum SSE (or maximum ESS) before, then estimating the uncertainty on the truncated data biases your uncertainty estimates downwards. This is especially true if you use a method which thoroughly accounts for autocorrelation for both equilibration detection and uncertainty estimation.</p>"},{"location":"reference/","title":"Index","text":""},{"location":"reference/#red","title":"red","text":"<p>Robust Equilibration Detection</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>red<ul> <li>confidence_intervals</li> <li>equilibration</li> <li>ess</li> <li>gelman_rubin</li> <li>plot</li> <li>sse</li> <li>variance</li> </ul> </li> </ul>"},{"location":"reference/confidence_intervals/","title":"confidence_intervals","text":""},{"location":"reference/confidence_intervals/#red.confidence_intervals","title":"confidence_intervals","text":"<p>Convenience function for computing 95 % confidence intervals.</p>"},{"location":"reference/confidence_intervals/#red.confidence_intervals.get_conf_int_init_seq","title":"get_conf_int_init_seq","text":"<pre><code>get_conf_int_init_seq(\n    data: NDArray[float64],\n    alpha_two_tailed: float = 0.05,\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n) -&gt; float\n</code></pre> <p>Calculate the confidence interval for the mean of a time series using initial sequence methods. See Geyer, 1992: https://www.jstor.org/stable/2246094.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>alpha_two_tailed</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The two-tailed significance level to use. The default is 0.05.</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The standard error of the mean.</p> </li> </ul> Source code in <code>red/confidence_intervals.py</code> <pre><code>def get_conf_int_init_seq(\n    data: _npt.NDArray[_np.float64],\n    alpha_two_tailed: float = 0.05,\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate the confidence interval for the mean of a time\n    series using initial sequence methods. See Geyer, 1992:\n    https://www.jstor.org/stable/2246094.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    alpha_two_tailed : float, optional\n        The two-tailed significance level to use. The default is 0.05.\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    Returns\n    -------\n    float\n        The standard error of the mean.\n    \"\"\"\n    # Get the correlated estimate of the variance.\n    var_cor, max_lag, acovf = _get_variance_initial_sequence(\n        data=data,\n        sequence_estimator=sequence_estimator,\n        min_max_lag_time=min_max_lag_time,\n        max_max_lag_time=max_max_lag_time,\n    )\n\n    # Get the standard error of the mean.\n    sem = _np.sqrt(var_cor / data.size)\n\n    # Get the effective sample size.\n    g = var_cor / acovf[0]\n    ess = data.size / g\n\n    # Raise a warning for low effective sample size.\n    if ess &lt; 50:  # Arbitrary, but matches pymbar timeseries\n        _warn(\n            f\"Effective sample size is low: {ess}. Confidence intervals may be unreliable.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # Get the 95 % confidence interval.\n    t_val = _t.ppf(1 - alpha_two_tailed / 2, ess - 1)\n\n    ci = t_val * sem\n\n    return ci  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/equilibration/","title":"equilibration","text":""},{"location":"reference/equilibration/#red.equilibration","title":"equilibration","text":"<p>Functions for selecting the equilibration time of a time series.</p>"},{"location":"reference/equilibration/#red.equilibration.detect_equilibration_init_seq","title":"detect_equilibration_init_seq","text":"<pre><code>detect_equilibration_init_seq(\n    data: NDArray[float64],\n    times: Optional[NDArray[float64]] = None,\n    method: str = \"min_sse\",\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n    plot: bool = False,\n    plot_name: Union[\n        str, Path\n    ] = \"equilibration_sse_init_seq.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n    plot_max_lags: bool = True,\n) -&gt; Tuple[Union[float, int], float, float]\n</code></pre> <p>Detect the equilibration time of a time series by finding the minimum squared standard error (SSE), or maximum effective sample size (ESS) of the time series, using initial sequence estimators of the variance. This is done by computing the SSE at each time point, discarding all samples before the time point. The index of the time point with the minimum SSE or maximum ESS is taken to be the point of equilibration.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) or (n_samples,).</p> </li> <li> <code>times</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The times at which the data was sampled. If this is not provided, the indices of the data will be used.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>\"min_sse\"</code> )           \u2013            <p>The method to use to select the equilibration time. This can be \"min_sse\" or \"max_ess\".</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> <li> <code>smooth_lag_times</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to smooth out the max lag times by a) converting them to a monotinically decreasing sequence and b) linearly interpolating between points where the sequence changes. This may be useful when the max lag times are noisy.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the SSE curve. The default is False.</p> </li> <li> <code>plot_name</code>               (<code>str | Path</code>, default:                   <code>'equilibration_sse_init_seq.png'</code> )           \u2013            <p>The name of the plot file. The default is 'equilibration_sse_init_seq.png'.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>data_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label for the time series data. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> <li> <code>plot_max_lags</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the maximum lag times used to estimate the variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>equil_time</code> (              <code>float | int</code> )          \u2013            <p>The time (or index, if no times are supplied) at which the time series is equilibrated.</p> </li> <li> <code>equil_g</code> (              <code>float</code> )          \u2013            <p>The statistical inefficiency at the equilibration point.</p> </li> <li> <code>equil_ess</code> (              <code>float</code> )          \u2013            <p>The effective sample size at the equilibration point.</p> </li> </ul> Source code in <code>red/equilibration.py</code> <pre><code>def detect_equilibration_init_seq(\n    data: _npt.NDArray[_np.float64],\n    times: _Optional[_npt.NDArray[_np.float64]] = None,\n    method: str = \"min_sse\",\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n    plot: bool = False,\n    plot_name: _Union[str, _Path] = \"equilibration_sse_init_seq.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n    plot_max_lags: bool = True,\n) -&gt; _Tuple[_Union[float, int], float, float]:\n    r\"\"\"\n    Detect the equilibration time of a time series by finding the minimum\n    squared standard error (SSE), or maximum effective sample size (ESS)\n    of the time series, using initial sequence estimators of the variance.\n    This is done by computing the SSE at each time point, discarding all\n    samples before the time point. The index of the time point with\n    the minimum SSE or maximum ESS is taken to be the point of equilibration.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) or (n_samples,).\n\n    times : np.ndarray, optional\n        The times at which the data was sampled. If this is\n        not provided, the indices of the data will be used.\n\n    method : str, optional, default=\"min_sse\"\n        The method to use to select the equilibration time. This can be\n        \"min_sse\" or \"max_ess\".\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    smooth_lag_times : bool, optional, default=False\n        Whether to smooth out the max lag times by a) converting them to a monotinically\n        decreasing sequence and b) linearly interpolating between points where the sequence\n        changes. This may be useful when the max lag times are noisy.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    plot : bool, optional\n        Whether to plot the SSE curve. The default is False.\n\n    plot_name : str | Path, optional\n        The name of the plot file. The default is 'equilibration_sse_init_seq.png'.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    data_y_label : str, optional\n        The y-axis label for the time series data. The default is\n        \"$\\Delta G$ / kcal mol$^{-1}$\".\n\n    plot_max_lags : bool, optional, default=True\n        Whether to plot the maximum lag times used to estimate the variance.\n\n    Returns\n    -------\n    equil_time: float | int\n        The time (or index, if no times are supplied) at which\n        the time series is equilibrated.\n\n    equil_g: float\n        The statistical inefficiency at the equilibration point.\n\n    equil_ess: float\n        The effective sample size at the equilibration point.\n    \"\"\"\n    # Check that data is valid.\n    data = check_data(data, one_dim_allowed=True)\n    _, n_samples = data.shape\n\n    # Check that method is valid.\n    valid_methods = [\"min_sse\", \"max_ess\"]\n    method = method.lower()\n    if method not in valid_methods:\n        raise InvalidInputError(f\"method must be one of {valid_methods}, but got {method}.\")\n\n    # If times is None, units of time are indices.\n    if times is None:\n        time_units = \"index\"\n        # Convert times to indices.\n        times_valid: _npt.NDArray[_np.float64] = _np.arange(n_samples, dtype=_np.float64)\n    else:\n        # To satisfy type checking.\n        times_valid = times\n\n    # Get the SSE timeseries.\n    sse_vals, max_lag_times = get_sse_series_init_seq(\n        data=data,\n        sequence_estimator=sequence_estimator,\n        min_max_lag_time=min_max_lag_time,\n        max_max_lag_time=max_max_lag_time,\n        smooth_lag_times=smooth_lag_times,\n        frac_padding=frac_padding,\n    )\n\n    # Get the corresponding times (or indices).\n    sse_times = times_valid[: len(sse_vals)]\n\n    # Convert the SSE to 1/ESS if requested (divide by uncorrelated variance).\n    if method == \"max_ess\":\n        ess_vals = convert_sse_series_to_ess_series(data=data, sse_series=sse_vals)\n        sse_vals = 1 / ess_vals\n\n    # Get the index of the minimum SSE.\n    equil_idx = _np.argmin(sse_vals)\n    equil_time = sse_times[equil_idx]\n\n    # Now compute the effective sample size at this time point.\n    equil_data = data[:, equil_idx:]\n    equil_ess = 1 / sse_vals[equil_idx]\n    if method == \"min_sse\":  # Has not yet been multiplied by the uncorrelated variance.\n        equil_ess *= equil_data.var()\n    equil_g = equil_data.size / equil_ess\n    equil_ess = equil_data.size / equil_g\n\n    if plot:\n        # Create a figure.\n        fig = _plt.figure(figsize=(6, 4))\n        gridspec_obj = _gridspec.GridSpec(1, 1, figure=fig)\n\n        # Plot the ESS.\n        plot_equilibration_min_sse(\n            fig=fig,\n            subplot_spec=gridspec_obj[0],\n            data=data,\n            sse_series=sse_vals,\n            max_lag_series=max_lag_times if plot_max_lags else None,\n            data_times=times_valid,\n            sse_times=sse_times,\n            time_units=time_units,\n            data_y_label=data_y_label,\n            variance_y_label=\"ESS\"\n            if method == \"max_ess\"\n            else r\"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n        )\n\n        fig.savefig(str(plot_name), dpi=300, bbox_inches=\"tight\")\n\n    # Return the equilibration index, limiting variance estimate, and SSE.\n    return equil_time, equil_g, equil_ess\n</code></pre>"},{"location":"reference/equilibration/#red.equilibration.detect_equilibration_window","title":"detect_equilibration_window","text":"<pre><code>detect_equilibration_window(\n    data: NDArray[float64],\n    times: Optional[NDArray[float64]] = None,\n    method: str = \"min_sse\",\n    kernel: Callable[\n        [int], NDArray[float64]\n    ] = _np.bartlett,\n    window_size_fn: Optional[\n        Callable[[int], int]\n    ] = lambda x: round(x**0.5),\n    window_size: Optional[int] = None,\n    frac_padding: float = 0.1,\n    plot: bool = False,\n    plot_name: Union[\n        str, Path\n    ] = \"equilibration_sse_window.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n    plot_window_size: bool = True,\n) -&gt; Tuple[Union[float, int], float, float]\n</code></pre> <p>Detect the equilibration time of a time series by finding the minimum squared standard error (SSE) or maximum effective sample size (ESS) of the time series, using window estimators of the variance. This is done by computing the SSE at each time point, discarding all samples before the time point. The index of the time point with the minimum SSE is taken to be the point of equilibration.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) or (n_samples,). If the method is 'lugsail', the data may have only one run, but otherwise there must be at least two runs.</p> </li> <li> <code>times</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The times at which the data was sampled. If this is not provided, the indices of the data will be used.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>\"min_sse\"</code> )           \u2013            <p>The method to use to select the equilibration time. This can be \"min_sse\" or \"max_ess\".</p> </li> <li> <code>kernel</code>               (<code>callable</code>, default:                   <code>numpy.bartlett</code> )           \u2013            <p>A function that takes a window size and returns a window function.</p> </li> <li> <code>window_size_fn</code>               (<code>callable</code>, default:                   <code>lambda x: round(x**0.5)</code> )           \u2013            <p>A function that takes the length of the time series and returns the window size to use. If this is not None, window_size must be None.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The size of the window to use, defined in terms of time lags in the forwards direction. If this is not None, window_size_fn must be None.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the ESS curve. The default is False.</p> </li> <li> <code>plot_name</code>               (<code>str | Path</code>, default:                   <code>'equilibration_sse_window.png'</code> )           \u2013            <p>The name of the plot file. The default is 'equilibration_sse_window.png'.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>data_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label for the time series data. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> <li> <code>plot_window_size</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the window size used to estimate the variance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>equil_time</code> (              <code>float | int</code> )          \u2013            <p>The time (or index, if no times are supplied) at which the time series is equilibrated.</p> </li> <li> <code>equil_g</code> (              <code>float</code> )          \u2013            <p>The statistical inefficiency at the equilibration point.</p> </li> <li> <code>equil_ess</code> (              <code>float</code> )          \u2013            <p>The effective sample size at the equilibration point.</p> </li> </ul> Source code in <code>red/equilibration.py</code> <pre><code>def detect_equilibration_window(\n    data: _npt.NDArray[_np.float64],\n    times: _Optional[_npt.NDArray[_np.float64]] = None,\n    method: str = \"min_sse\",\n    kernel: _Callable[[int], _npt.NDArray[_np.float64]] = _np.bartlett,  # type: ignore\n    window_size_fn: _Optional[_Callable[[int], int]] = lambda x: round(x**0.5),\n    window_size: _Optional[int] = None,\n    frac_padding: float = 0.1,\n    plot: bool = False,\n    plot_name: _Union[str, _Path] = \"equilibration_sse_window.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n    plot_window_size: bool = True,\n) -&gt; _Tuple[_Union[float, int], float, float]:\n    r\"\"\"\n    Detect the equilibration time of a time series by finding the minimum\n    squared standard error (SSE) or maximum effective sample size (ESS)\n    of the time series, using window estimators of the variance. This is\n    done by computing the SSE at each time point, discarding all samples\n    before the time point. The index of the time point with the minimum\n    SSE is taken to be the point of equilibration.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) or (n_samples,). If the method\n        is 'lugsail', the data may have only one run, but\n        otherwise there must be at least two runs.\n\n    times : np.ndarray, optional\n        The times at which the data was sampled. If this is\n        not provided, the indices of the data will be used.\n\n    method : str, optional, default=\"min_sse\"\n        The method to use to select the equilibration time. This can be\n        \"min_sse\" or \"max_ess\".\n\n    kernel : callable, optional, default=numpy.bartlett\n        A function that takes a window size and returns a window function.\n\n    window_size_fn : callable, optional, default=lambda x: round(x**0.5)\n        A function that takes the length of the time series and returns the window size\n        to use. If this is not None, window_size must be None.\n\n    window_size : int, optional, default=None\n        The size of the window to use, defined in terms of time lags in the\n        forwards direction. If this is not None, window_size_fn must be None.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    plot : bool, optional\n        Whether to plot the ESS curve. The default is False.\n\n    plot_name : str | Path, optional\n        The name of the plot file. The default is 'equilibration_sse_window.png'.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    data_y_label : str, optional\n        The y-axis label for the time series data. The default is\n        \"$\\Delta G$ / kcal mol$^{-1}$\".\n\n    plot_window_size : bool, optional, default=True\n        Whether to plot the window size used to estimate the variance.\n\n    Returns\n    -------\n    equil_time: float | int\n        The time (or index, if no times are supplied) at which\n        the time series is equilibrated.\n\n    equil_g: float\n        The statistical inefficiency at the equilibration point.\n\n    equil_ess: float\n        The effective sample size at the equilibration point.\n    \"\"\"\n    # Check that data is valid.\n    data = check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n\n    # If times is None, units of time are indices.\n    if times is None:\n        time_units = \"index\"\n        # Convert times to indices.\n        times_valid: _npt.NDArray[_np.float64] = _np.arange(n_samples, dtype=_np.float64)\n    else:\n        # To satisfy type checking.\n        times_valid = times\n\n    # Get the SSE timeseries\n    sse_vals, window_sizes = get_sse_series_window(\n        data=data,\n        kernel=kernel,\n        window_size_fn=window_size_fn,\n        window_size=window_size,\n        frac_padding=frac_padding,\n    )\n\n    # Get the corresponding times (or indices).\n    sse_times = times_valid[: len(sse_vals)]  # type: ignore\n\n    # Convert the SSE to 1/ESS if requested (divide by uncorrelated variance).\n    if method == \"max_ess\":\n        ess_vals = convert_sse_series_to_ess_series(data=data, sse_series=sse_vals)\n        sse_vals = 1 / ess_vals\n\n    # Get the index of the minimum SSE.\n    equil_idx = _np.argmin(sse_vals)\n    equil_time = sse_times[equil_idx]\n\n    # Now compute the effective sample size at this time point.\n    equil_data = data[:, equil_idx:]\n    equil_ess = 1 / sse_vals[equil_idx]\n    if method == \"min_sse\":  # Has not yet been multiplied by the uncorrelated variance.\n        equil_ess *= equil_data.var()\n    equil_g = equil_data.size / equil_ess\n    equil_ess = equil_data.size / equil_g\n\n    if plot:\n        # Create a figure.\n        fig = _plt.figure(figsize=(6, 4))\n        gridspec_obj = _gridspec.GridSpec(1, 1, figure=fig)\n\n        # Plot the ESS.\n        plot_equilibration_min_sse(\n            fig=fig,\n            subplot_spec=gridspec_obj[0],\n            data=data,\n            sse_series=sse_vals,\n            data_times=times_valid,\n            sse_times=sse_times,\n            time_units=time_units,\n            data_y_label=data_y_label,\n            window_size_series=window_sizes if plot_window_size else None,\n            variance_y_label=\"ESS\"\n            if method == \"max_ess\"\n            else r\"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n        )\n\n        fig.savefig(str(plot_name), dpi=300, bbox_inches=\"tight\")\n\n    # Return the equilibration time (index), statistical inefficiency, and ESS.\n    return equil_time, equil_g, equil_ess\n</code></pre>"},{"location":"reference/equilibration/#red.equilibration.get_paired_t_p_timeseries","title":"get_paired_t_p_timeseries","text":"<pre><code>get_paired_t_p_timeseries(\n    data: NDArray[float64],\n    times: Optional[NDArray[float64]] = None,\n    fractional_block_size: float = 0.125,\n    fractional_test_end: float = 0.5,\n    initial_block_size: float = 0.1,\n    final_block_size: float = 0.5,\n    t_test_sidedness: str = \"two-sided\",\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Get a timeseries of the p-values from a paired t-test on the differences between sample means between intial and final portions of the data. The timeseries is obtained by repeatedly discarding more data from the time series between calculations of the p-value.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> <li> <code>times</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The times at which the data was sampled. If this is not provided, the indices of the data will be used.</p> </li> <li> <code>fractional_block_size</code>               (<code>float</code>, default:                   <code>0.125</code> )           \u2013            <p>The fraction of data to discard between repeats. The default is 0.125.</p> </li> <li> <code>fractional_test_end</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The fraction of the time series to use in the final test. The default is 0.5.</p> </li> <li> <code>initial_block_size</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the truncated time series to use for the \"before\" portion of the paired t-test. The default is 0.1.</p> </li> <li> <code>final_block_size</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The fraction of the truncated time series to use for the \"after\" portion of the paired t-test. The default is 0.5.</p> </li> <li> <code>t_test_sidedness</code>               (<code>str</code>, default:                   <code>'two-sided'</code> )           \u2013            <p>The sidedness of the paired t-test. This can be either 'two-sided', 'less', or 'greater'. The default is 'two-sided'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The p-values of the paired t-test.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The times at which the p-values were calculated.</p> </li> </ul> Source code in <code>red/equilibration.py</code> <pre><code>def get_paired_t_p_timeseries(\n    data: _npt.NDArray[_np.float64],\n    times: _Optional[_npt.NDArray[_np.float64]] = None,\n    fractional_block_size: float = 0.125,\n    fractional_test_end: float = 0.5,\n    initial_block_size: float = 0.1,\n    final_block_size: float = 0.5,\n    t_test_sidedness: str = \"two-sided\",\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Get a timeseries of the p-values from a paired t-test on the differences\n    between sample means between intial and final portions of the data. The timeseries\n    is obtained by repeatedly discarding more data from the time series between\n    calculations of the p-value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    times : np.ndarray, optional\n        The times at which the data was sampled. If this is\n        not provided, the indices of the data will be used.\n\n    fractional_block_size : float, optional\n        The fraction of data to discard between repeats. The default is 0.125.\n\n    fractional_test_end : float, optional\n        The fraction of the time series to use in the final test. The default is 0.5.\n\n    initial_block_size : float, optional\n        The fraction of the truncated time series to use for the \"before\" portion\n        of the paired t-test. The default is 0.1.\n\n    final_block_size : float, optional\n        The fraction of the truncated time series to use for the \"after\" portion\n        of the paired t-test. The default is 0.5.\n\n    t_test_sidedness : str, optional\n        The sidedness of the paired t-test. This can be either 'two-sided', 'less',\n        or 'greater'. The default is 'two-sided'.\n\n    Returns\n    -------\n    np.ndarray\n        The p-values of the paired t-test.\n\n    np.ndarray\n        The times at which the p-values were calculated.\n    \"\"\"\n    # Check that the data is valid.\n    data = check_data(data, one_dim_allowed=False)\n    n_runs, n_samples = data.shape\n\n    # Convert times to indices if necessary.\n    if times is None:\n        times = _np.arange(n_samples, dtype=_np.float64)\n\n    # Check that times is match the number of samples.\n    if n_samples != len(times):\n        raise InvalidInputError(\"Times must have the same length as the number of samples.\")\n\n    # Check that user inputs are valid.\n    if fractional_block_size &lt;= 0 or fractional_block_size &gt; 1:\n        raise InvalidInputError(\"fractional_block_size must be between 0 and 1.\")\n\n    if fractional_test_end &lt;= 0 or fractional_test_end &gt; 1:\n        raise InvalidInputError(\"fractional_test_end must be between 0 and 1.\")\n\n    if round(fractional_test_end / fractional_block_size) &lt; 2:\n        raise InvalidInputError(\n            \"fractional_test_end must be at least twice the fractional_block_size.\"\n        )\n\n    # Check that fractional test end is a multiple of fractional block size.\n    if round((fractional_test_end / fractional_block_size) % 1.0, 3) != 0:\n        raise InvalidInputError(\"fractional_test_end must be a multiple of fractional_block_size.\")\n\n    if initial_block_size &lt;= 0 or initial_block_size &gt; 1:\n        raise InvalidInputError(\"initial_block_size must be between 0 and 1.\")\n\n    if final_block_size &lt;= 0 or final_block_size &gt; 1:\n        raise InvalidInputError(\"final_block_size must be between 0 and 1.\")\n\n    t_test_sidedness = t_test_sidedness.lower()\n    if t_test_sidedness not in [\"two-sided\", \"less\", \"greater\"]:\n        raise InvalidInputError(\n            \"t_test_sidedness must be either 'two-sided', 'less', or 'greater'.\"\n        )\n\n    # Calculate the number of repeats.\n    n_repeats = round(fractional_test_end / fractional_block_size) + 1  # + 1 for the initial block\n\n    # Calculate the number of samples to discard between repeats.\n    n_discard = round(n_samples * fractional_block_size)\n\n    # Calculate the p values with their indices.\n    p_vals = _np.zeros(n_repeats)\n    p_val_indices = _np.zeros(n_repeats, dtype=int)\n    time_vals = _np.zeros(n_repeats)\n\n    # Loop over and calculate the p values.\n    for i in range(n_repeats):\n        # Truncate the data.\n        idx = n_discard * i\n        truncated_data = data[:, idx:]\n        # Get the number of samples in the truncated data.\n        n_truncated_samples = truncated_data.shape[1]\n        # Get the initial and final blocks.\n        initial_block = truncated_data[:, : round(n_truncated_samples * initial_block_size)].mean(\n            axis=1\n        )\n        final_block = truncated_data[:, -round(n_truncated_samples * final_block_size) :].mean(\n            axis=1\n        )\n        # Compute the paired t-test.\n        p_vals[i] = _ttest_rel(initial_block, final_block, alternative=t_test_sidedness)[1]\n        p_val_indices[i] = idx\n        time_vals[i] = times[idx]\n\n    return p_vals, time_vals\n</code></pre>"},{"location":"reference/equilibration/#red.equilibration.detect_equilibration_paired_t_test","title":"detect_equilibration_paired_t_test","text":"<pre><code>detect_equilibration_paired_t_test(\n    data: NDArray[float64],\n    times: Optional[NDArray[float64]] = None,\n    p_threshold: float = 0.05,\n    fractional_block_size: float = 0.125,\n    fractional_test_end: float = 0.5,\n    initial_block_size: float = 0.1,\n    final_block_size: float = 0.5,\n    t_test_sidedness: str = \"two-sided\",\n    plot: bool = False,\n    plot_name: Union[\n        str, Path\n    ] = \"equilibration_paired_t_test.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; int\n</code></pre> <p>Detect the equilibration time of a time series by performing a paired t-test between initial and final portions of the time series. This is repeated , discarding more data from the time series between repeats. If the p-value is greater than the threshold, there is no significant evidence that the data is no equilibrated and the timeseries is taken to be equilibrated at this time point. This test may be useful when we care only about systematic bias in the data, and do not care about detecting inter-run differences.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> <li> <code>times</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The times at which the data was sampled. If this is not provided, the indices of the data will be used.</p> </li> <li> <code>p_threshold</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The p-value threshold to use. The default is 0.05.</p> </li> <li> <code>fractional_block_size</code>               (<code>float</code>, default:                   <code>0.125</code> )           \u2013            <p>The fraction of data to discard between repeats. The default is 0.125.</p> </li> <li> <code>fractional_test_end</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The fraction of the time series to use in the final test. The default is 0.5.</p> </li> <li> <code>initial_block_size</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the truncated time series to use for the \"before\" portion of the paired t-test. The default is 0.1.</p> </li> <li> <code>final_block_size</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The fraction of the truncated time series to use for the \"after\" portion of the paired t-test. The default is 0.5.</p> </li> <li> <code>t_test_sidedness</code>               (<code>str</code>, default:                   <code>'two-sided'</code> )           \u2013            <p>The sidedness of the paired t-test. This can be either 'two-sided', 'less', or 'greater'. The default is 'two-sided'.</p> </li> <li> <code>plot</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to plot the p-values. The default is False.</p> </li> <li> <code>plot_name</code>               (<code>str | Path</code>, default:                   <code>'equilibration_paired_t_test.png'</code> )           \u2013            <p>The name of the plot file. The default is 'equilibration_paired_t_test.png'.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>data_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label for the time series data. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The time point at which the time series is equilibrated.</p> </li> </ul> Source code in <code>red/equilibration.py</code> <pre><code>def detect_equilibration_paired_t_test(\n    data: _npt.NDArray[_np.float64],\n    times: _Optional[_npt.NDArray[_np.float64]] = None,\n    p_threshold: float = 0.05,\n    fractional_block_size: float = 0.125,\n    fractional_test_end: float = 0.5,\n    initial_block_size: float = 0.1,\n    final_block_size: float = 0.5,\n    t_test_sidedness: str = \"two-sided\",\n    plot: bool = False,\n    plot_name: _Union[str, _Path] = \"equilibration_paired_t_test.png\",\n    time_units: str = \"ns\",\n    data_y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; int:\n    r\"\"\"\n    Detect the equilibration time of a time series by performing a paired\n    t-test between initial and final portions of the time series. This is repeated\n    , discarding more data from the time series between repeats. If the p-value\n    is greater than the threshold, there is no significant evidence that the data is\n    no equilibrated and the timeseries is taken to be equilibrated at this time\n    point. This test may be useful when we care only about systematic bias in the\n    data, and do not care about detecting inter-run differences.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    times : np.ndarray, optional\n        The times at which the data was sampled. If this is\n        not provided, the indices of the data will be used.\n\n    p_threshold : float, optional\n        The p-value threshold to use. The default is 0.05.\n\n    fractional_block_size : float, optional\n        The fraction of data to discard between repeats. The default is 0.125.\n\n    fractional_test_end : float, optional\n        The fraction of the time series to use in the final test. The default is 0.5.\n\n    initial_block_size : float, optional\n        The fraction of the truncated time series to use for the \"before\" portion\n        of the paired t-test. The default is 0.1.\n\n    final_block_size : float, optional\n        The fraction of the truncated time series to use for the \"after\" portion\n        of the paired t-test. The default is 0.5.\n\n    t_test_sidedness : str, optional\n        The sidedness of the paired t-test. This can be either 'two-sided', 'less',\n        or 'greater'. The default is 'two-sided'.\n\n    plot : bool, optional\n        Whether to plot the p-values. The default is False.\n\n    plot_name : str | Path, optional\n        The name of the plot file. The default is 'equilibration_paired_t_test.png'.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    data_y_label : str, optional\n        The y-axis label for the time series data. The default is\n        \"$\\Delta G$ / kcal mol$^{-1}$\".\n\n    Returns\n    -------\n    int\n        The time point at which the time series is equilibrated.\n    \"\"\"\n    # Validate dtata.\n    data = check_data(data, one_dim_allowed=False)\n    n_runs, n_samples = data.shape\n\n    # If times is None, units of time are indices.\n    if times is None:\n        time_units = \"index\"\n        # Convert times to indices.\n        times = _np.arange(n_samples, dtype=_np.float64)\n\n    # Check that user options (not checked in get_paired_t_p_timeseries) are valid.\n    if p_threshold &lt;= 0 or p_threshold &gt; 0.7:\n        raise InvalidInputError(\"p_threshold must be between 0 and 0.7.\")\n\n    # Get the p value timeseries and n_discard.\n    p_vals, times_used = get_paired_t_p_timeseries(\n        data=data,\n        times=times,\n        fractional_block_size=fractional_block_size,\n        fractional_test_end=fractional_test_end,\n        initial_block_size=initial_block_size,\n        final_block_size=final_block_size,\n        t_test_sidedness=t_test_sidedness,\n    )\n\n    # Get the index of the first p value that is greater than the threshold.\n    meets_threshold = p_vals &gt; p_threshold\n    if not any(meets_threshold):\n        raise EquilibrationNotDetectedError(\n            f\"No p values are greater than the threshold of {p_threshold}.\"\n        )\n    equil_time = times_used[_np.argmax(meets_threshold)]\n\n    # Plot the p values.\n    if plot:\n        fig = _plt.figure(figsize=(6, 4))\n        gridspec_obj = _gridspec.GridSpec(1, 1, figure=fig)\n        plot_equilibration_paired_t_test(\n            fig=fig,\n            subplot_spec=gridspec_obj[0],\n            data=data,\n            p_values=p_vals,\n            data_times=times,\n            p_times=times_used,\n            p_threshold=p_threshold,\n            time_units=time_units,\n            data_y_label=data_y_label,\n        )\n\n        fig.savefig(str(plot_name), dpi=300, bbox_inches=\"tight\")\n\n    return equil_time  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/ess/","title":"ess","text":""},{"location":"reference/ess/#red.ess","title":"ess","text":"<p>Functions to calculate the statistical inefficiency and effective sample size.</p>"},{"location":"reference/ess/#red.ess.convert_sse_series_to_ess_series","title":"convert_sse_series_to_ess_series","text":"<pre><code>convert_sse_series_to_ess_series(\n    data: NDArray[float64], sse_series: NDArray[float64]\n) -&gt; NDArray[float64]\n</code></pre> <p>Convert a series of squared standard errors to a series of effective sample sizes.</p> <p>Parameters:</p> <ul> <li> <code>sse_series</code>               (<code>ndarray</code>)           \u2013            <p>The squared standard error series.</p> </li> <li> <code>uncor_vars</code>               (<code>ndarray</code>)           \u2013            <p>The uncorrelated variances.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The effective sample size series.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def convert_sse_series_to_ess_series(\n    data: _npt.NDArray[_np.float64], sse_series: _npt.NDArray[_np.float64]\n) -&gt; _npt.NDArray[_np.float64]:\n    \"\"\"\n    Convert a series of squared standard errors to a series of effective sample sizes.\n\n    Parameters\n    ----------\n    sse_series : np.ndarray\n        The squared standard error series.\n\n    uncor_vars : np.ndarray\n        The uncorrelated variances.\n\n    Returns\n    -------\n    np.ndarray\n        The effective sample size series.\n    \"\"\"\n    # Validate the data.\n    data = check_data(data, one_dim_allowed=True)\n\n    # Now get the uncorrelated variances.\n    uncor_vars = _np.zeros_like(sse_series)\n\n    for i in range(len(sse_series)):\n        # Get \"biased\", rather than n - 1, variance.\n        uncor_vars[i] = data[:, i:].var()  # type: ignore\n\n    return uncor_vars / sse_series  # type: ignore\n</code></pre>"},{"location":"reference/ess/#red.ess.get_ess_series_init_seq","title":"get_ess_series_init_seq","text":"<pre><code>get_ess_series_init_seq(\n    data: NDArray[float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Compute a series of effective sample sizes for a time series as data is discarded from the beginning of the time series. The autocorrelation is computed using the sequence estimator specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> <li> <code>smooth_lag_times</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to smooth out the max lag times by a) converting them to a monotinically decreasing sequence and b) linearly interpolating between points where the sequence changes. This may be useful when the max lag times are noisy.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The effective sample size series.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The maximum lag times used.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def get_ess_series_init_seq(\n    data: _npt.NDArray[_np.float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Compute a series of effective sample sizes for a time series as data\n    is discarded from the beginning of the time series. The autocorrelation\n    is computed using the sequence estimator specified.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    smooth_lag_times : bool, optional, default=False\n        Whether to smooth out the max lag times by a) converting them to a monotinically\n        decreasing sequence and b) linearly interpolating between points where the sequence\n        changes. This may be useful when the max lag times are noisy.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    Returns\n    -------\n    np.ndarray\n        The effective sample size series.\n\n    np.ndarray\n        The maximum lag times used.\n    \"\"\"\n    sse_series, max_lag_times = _get_sse_series_init_seq(\n        data,\n        sequence_estimator=sequence_estimator,\n        min_max_lag_time=min_max_lag_time,\n        max_max_lag_time=max_max_lag_time,\n        smooth_lag_times=smooth_lag_times,\n        frac_padding=frac_padding,\n    )\n\n    ess_series = convert_sse_series_to_ess_series(data, sse_series)\n\n    return ess_series, max_lag_times\n</code></pre>"},{"location":"reference/ess/#red.ess.get_ess_series_window","title":"get_ess_series_window","text":"<pre><code>get_ess_series_window(\n    data: NDArray[float64],\n    kernel: Callable[\n        [int], NDArray[float64]\n    ] = _np.bartlett,\n    window_size_fn: Optional[\n        Callable[[int], int]\n    ] = lambda x: round(x**0.5),\n    window_size: Optional[int] = None,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Compute a series of effective sample sizes for a time series as data is discarded from the beginning of the time series. The squared standard error is computed using the window size and kernel specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>kernel</code>               (<code>callable</code>, default:                   <code>numpy.bartlett</code> )           \u2013            <p>A function that takes a window size and returns a window function.</p> </li> <li> <code>window_size_fn</code>               (<code>callable</code>, default:                   <code>lambda x: round(x**0.5)</code> )           \u2013            <p>A function that takes the length of the time series and returns the window size to use. If this is not None, window_size must be None.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The size of the window to use, defined in terms of time lags in the forwards direction. If this is not None, window_size_fn must be None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The squared standard error series.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The window sizes used.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def get_ess_series_window(\n    data: _npt.NDArray[_np.float64],\n    kernel: _Callable[[int], _npt.NDArray[_np.float64]] = _np.bartlett,  # type: ignore\n    window_size_fn: _Optional[_Callable[[int], int]] = lambda x: round(x**0.5),\n    window_size: _Optional[int] = None,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Compute a series of effective sample sizes for a time series as data\n    is discarded from the beginning of the time series. The squared standard\n    error is computed using the window size and kernel specified.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    kernel : callable, optional, default=numpy.bartlett\n        A function that takes a window size and returns a window function.\n\n    window_size_fn : callable, optional, default=lambda x: round(x**0.5)\n        A function that takes the length of the time series and returns the window size\n        to use. If this is not None, window_size must be None.\n\n    window_size : int, optional, default=None\n        The size of the window to use, defined in terms of time lags in the\n        forwards direction. If this is not None, window_size_fn must be None.\n\n    Returns\n    -------\n    np.ndarray\n        The squared standard error series.\n\n    np.ndarray\n        The window sizes used.\n    \"\"\"\n    sse_series, max_lag_times = _get_sse_series_window(\n        data, kernel=kernel, window_size_fn=window_size_fn, window_size=window_size\n    )\n\n    ess_series = convert_sse_series_to_ess_series(data, sse_series)\n\n    return ess_series, max_lag_times\n</code></pre>"},{"location":"reference/ess/#red.ess.statistical_inefficiency_inter_variance","title":"statistical_inefficiency_inter_variance","text":"<pre><code>statistical_inefficiency_inter_variance(\n    data: NDArray[float64],\n) -&gt; float\n</code></pre> <p>Compute the statistical inefficiency of a time series by dividing the inter-run variance estimate by the intra-run variance estimate. More than one run is required.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The statistical inefficiency.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def statistical_inefficiency_inter_variance(data: _npt.NDArray[_np.float64]) -&gt; float:\n    \"\"\"\n    Compute the statistical inefficiency of a time series by dividing\n    the inter-run variance estimate by the intra-run variance estimate.\n    More than one run is required.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    Returns\n    -------\n    float\n        The statistical inefficiency.\n    \"\"\"\n    g = inter_run_variance(data) / intra_run_variance(data)\n    # Ensure that the statistical inefficiency is at least 1.\n    return max(g, 1)\n</code></pre>"},{"location":"reference/ess/#red.ess.statistical_inefficiency_lugsail_variance","title":"statistical_inefficiency_lugsail_variance","text":"<pre><code>statistical_inefficiency_lugsail_variance(\n    data: NDArray[float64], n_pow: float = 1 / 3\n) -&gt; float\n</code></pre> <p>Compute the statistical inefficiency of a time series by dividing the lugsail replicated batch means variance estimate by the intra-run variance estimate. This is applicable to a single run.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> <li> <code>n_pow</code>               (<code>float</code>, default:                   <code>1 / 3</code> )           \u2013            <p>The power to use in the lugsail variance estimate. This should be between 0 and 1. The default is 1/3.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The statistical inefficiency.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def statistical_inefficiency_lugsail_variance(\n    data: _npt.NDArray[_np.float64], n_pow: float = 1 / 3\n) -&gt; float:\n    \"\"\"\n    Compute the statistical inefficiency of a time series by dividing\n    the lugsail replicated batch means variance estimate by the\n    intra-run variance estimate. This is applicable to a single run.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    n_pow : float, optional\n        The power to use in the lugsail variance estimate. This\n        should be between 0 and 1. The default is 1/3.\n\n    Returns\n    -------\n    float\n        The statistical inefficiency.\n    \"\"\"\n    g = lugsail_variance(data, n_pow=n_pow) / intra_run_variance(data)\n    # Ensure that the statistical inefficiency is at least 1.\n    return max(g, 1)\n</code></pre>"},{"location":"reference/ess/#red.ess.ess_inter_variance","title":"ess_inter_variance","text":"<pre><code>ess_inter_variance(data: NDArray[float64]) -&gt; float\n</code></pre> <p>Compute the effective sample size of a time series by dividing the total number of samples by the statistical inefficiency, where the statistical inefficiency is calculated using the ratio of the inter-run and intra-run variance estimates.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The effective sample size.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def ess_inter_variance(data: _npt.NDArray[_np.float64]) -&gt; float:\n    \"\"\"\n    Compute the effective sample size of a time series by dividing\n    the total number of samples by the statistical inefficiency, where\n    the statistical inefficiency is calculated using the ratio of the\n    inter-run and intra-run variance estimates.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    Returns\n    -------\n    float\n        The effective sample size.\n    \"\"\"\n    data = check_data(data, one_dim_allowed=False)\n    n_runs, n_samples = data.shape\n    total_samples = n_runs * n_samples\n    return total_samples / statistical_inefficiency_inter_variance(data)\n</code></pre>"},{"location":"reference/ess/#red.ess.ess_lugsail_variance","title":"ess_lugsail_variance","text":"<pre><code>ess_lugsail_variance(\n    data: NDArray[float64], n_pow: float = 1 / 3\n) -&gt; float\n</code></pre> <p>Compute the effective sample size of a time series by dividing the total number of samples by the statistical inefficiency, where the statistical inefficiency is calculated using the ratio of the lugsail replicated batch means and intra-run variance estimates.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> <li> <code>n_pow</code>               (<code>float</code>, default:                   <code>1 / 3</code> )           \u2013            <p>The power to use in the lugsail variance estimate. This should be between 0 and 1. The default is 1/3.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The effective sample size.</p> </li> </ul> Source code in <code>red/ess.py</code> <pre><code>def ess_lugsail_variance(data: _npt.NDArray[_np.float64], n_pow: float = 1 / 3) -&gt; float:\n    \"\"\"\n    Compute the effective sample size of a time series by dividing\n    the total number of samples by the statistical inefficiency, where\n    the statistical inefficiency is calculated using the ratio of the\n    lugsail replicated batch means and intra-run variance estimates.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    n_pow : float, optional\n        The power to use in the lugsail variance estimate. This\n        should be between 0 and 1. The default is 1/3.\n\n    Returns\n    -------\n    float\n        The effective sample size.\n    \"\"\"\n    data = check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n    total_samples = n_runs * n_samples\n    return total_samples / statistical_inefficiency_lugsail_variance(data, n_pow=n_pow)\n</code></pre>"},{"location":"reference/gelman_rubin/","title":"gelman_rubin","text":""},{"location":"reference/gelman_rubin/#red.gelman_rubin","title":"gelman_rubin","text":"<p>Compute the Gelman-Rubin diagnostic.</p>"},{"location":"reference/gelman_rubin/#red.gelman_rubin.gelman_rubin","title":"gelman_rubin","text":"<pre><code>gelman_rubin(data: NDArray[float64]) -&gt; float\n</code></pre> <p>Compute the Gelman-Rubin diagnostic according to equation 4 in  Statist. Sci. 36(4): 518-529 (November 2021). DOI: 10.1214/20-STS812</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples) and must have at least two runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The Gelman-Rubin diagnostic.</p> </li> </ul> Source code in <code>red/gelman_rubin.py</code> <pre><code>def gelman_rubin(data: _npt.NDArray[_np.float64]) -&gt; float:\n    \"\"\"\n    Compute the Gelman-Rubin diagnostic according to\n    equation 4 in  Statist. Sci. 36(4): 518-529\n    (November 2021). DOI: 10.1214/20-STS812\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples) and must have at least two\n        runs.\n\n    Returns\n    -------\n    float\n        The Gelman-Rubin diagnostic.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=False)\n    _, n_samples = data.shape\n\n    # Compute the variance estimates.\n    intra_run_variance_est = _intra_run_variance(data)\n    inter_run_variance_est = _inter_run_variance(data)\n\n    # Get combined variance estimate.\n    combined_variance_est = (\n        n_samples - 1\n    ) / n_samples * intra_run_variance_est + inter_run_variance_est / n_samples\n\n    # Compute GR diagnostic.\n    gelman_rubin_diagnostic = _np.sqrt(combined_variance_est / intra_run_variance_est)\n\n    return gelman_rubin_diagnostic  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/gelman_rubin/#red.gelman_rubin.stable_gelman_rubin","title":"stable_gelman_rubin","text":"<pre><code>stable_gelman_rubin(\n    data: NDArray[float64], n_pow: float = 1 / 3\n) -&gt; float\n</code></pre> <p>Compute the stable Gelman-Rubin diagnostic according to equation 7 in  Statist. Sci. 36(4): 518-529 (November 2021). DOI: 10.1214/20-STS812. This is applicable to a single run.</p> Source code in <code>red/gelman_rubin.py</code> <pre><code>def stable_gelman_rubin(data: _npt.NDArray[_np.float64], n_pow: float = 1 / 3) -&gt; float:\n    \"\"\"\n    Compute the stable Gelman-Rubin diagnostic according to\n    equation 7 in  Statist. Sci. 36(4): 518-529\n    (November 2021). DOI: 10.1214/20-STS812. This is applicable to\n    a single run.\n    \"\"\"\n    # Validate the data.\n    data = _check_data(data, one_dim_allowed=True)\n    _, n_samples = data.shape\n\n    # Compute the variance estimates.\n    intra_run_variance_est = _intra_run_variance(data)\n    lugsail_variance_est = _lugsail_variance(data, n_pow=n_pow)\n\n    # Get combined variance estimate.\n    combined_variance_est = (\n        n_samples - 1\n    ) / n_samples * intra_run_variance_est + lugsail_variance_est / n_samples\n\n    # Compute GR diagnostic.\n    gelman_rubin_diagnostic = _np.sqrt(combined_variance_est / intra_run_variance_est)\n\n    return gelman_rubin_diagnostic  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/plot/","title":"plot","text":""},{"location":"reference/plot/#red.plot","title":"plot","text":"<p>Plotting functions.</p>"},{"location":"reference/plot/#red.plot.plot_timeseries","title":"plot_timeseries","text":"<pre><code>plot_timeseries(\n    ax: Axes,\n    data: NDArray[float64],\n    times: NDArray[float64],\n    n_blocks: int = 100,\n    time_units: str = \"ns\",\n    y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; None\n</code></pre> <p>Plot the (multi-run) time series data.</p> <p>Parameters:</p> <ul> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>The axes to plot on.</p> </li> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples), or (n_samples,) if there is only one run.</p> </li> <li> <code>times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the data was sampled. This should have shape (n_samples,).</p> </li> <li> <code>n_blocks</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The number of blocks to use for block averaging. This makes trends clearer. If 0, no block averaging is performed.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> </ul> Source code in <code>red/plot.py</code> <pre><code>def plot_timeseries(\n    ax: _Axes,\n    data: _npt.NDArray[_np.float64],\n    times: _npt.NDArray[_np.float64],\n    n_blocks: int = 100,\n    time_units: str = \"ns\",\n    y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; None:\n    r\"\"\"\n    Plot the (multi-run) time series data.\n\n    Parameters\n    ----------\n    ax : Axes\n        The axes to plot on.\n\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples), or (n_samples,) if there\n        is only one run.\n\n    times : np.ndarray\n        The times at which the data was sampled. This\n        should have shape (n_samples,).\n\n    n_blocks : int, optional\n        The number of blocks to use for block averaging. This\n        makes trends clearer. If 0, no block averaging is\n        performed.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    y_label : str, optional\n        The y-axis label. The default is \"$\\Delta G$ / kcal mol$^{-1}$\".\n    \"\"\"\n    # Check that data is valid.\n    data = check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n\n    # Check that times is valid.\n    if not isinstance(times, _np.ndarray):\n        raise InvalidInputError(\"Times must be a numpy array.\")\n\n    if times.ndim != 1:\n        raise InvalidInputError(\"Times must be one dimensional.\")\n\n    if times.shape[0] != n_samples:\n        raise InvalidInputError(\"Times must have the same length as the number of samples.\")\n\n    if n_blocks &lt; 0 or n_blocks &gt; n_samples:\n        raise InvalidInputError(\n            \"n_blocks must be greater than or equal to 0 and less than or equal to\"\n            \" the number of samples.\"\n        )\n\n    if n_blocks == 0:\n        n_blocks = n_samples\n\n    # Get the block size based on the number of blocks requested.\n    block_size = n_samples // n_blocks\n    # Pick the final sample based on the block size, rather than the number of blocks.\n    # We might end up with more blocks than requested, but this is better than failing\n    # to show too much of the data.\n    n_samples_end = n_samples - (n_samples % block_size)\n    # Update the number of blocks according to the block size.\n    n_blocks = n_samples_end // block_size\n    # Trim the data and times so that they are divisible by the block size.\n    times = times[:n_samples_end]\n    data = data[:, :n_samples_end]\n    data = data.reshape(n_runs, n_blocks, block_size).mean(axis=2)  # type: ignore\n    times = times.reshape(n_blocks, block_size).mean(axis=1)\n\n    # Decide the transparency of the individual run lines.\n    alpha_runs = 1.0 if n_runs == 1 else 0.5\n\n    # Plot the data.\n    for i in range(n_runs):\n        label = None if n_runs == 1 else f\"Run {i + 1}\"\n        ax.plot(times, data[i, :], alpha=alpha_runs, label=label)\n\n    # If we have more than one run, plot the mean.\n    if n_runs &gt; 1:\n        ax.plot(times, data.mean(axis=0), color=\"black\", label=\"Mean\")\n\n    # Only show the legend if there is more than one run.\n    if n_runs &gt; 1:\n        ax.legend()\n\n    # Set the axis labels.\n    ax.set_xlabel(f\"Time / {time_units}\")\n    ax.set_ylabel(y_label)\n</code></pre>"},{"location":"reference/plot/#red.plot.plot_p_values","title":"plot_p_values","text":"<pre><code>plot_p_values(\n    ax: Axes,\n    p_values: NDArray[float64],\n    times: NDArray[float64],\n    p_threshold: float = 0.05,\n    time_units: str = \"ns\",\n    threshold_times: Optional[NDArray[float64]] = None,\n) -&gt; None\n</code></pre> <p>Plot the p-values of the paired t-test.</p> <p>Parameters:</p> <ul> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>The axes to plot on.</p> </li> <li> <code>p_values</code>               (<code>ndarray</code>)           \u2013            <p>The p-values of the paired t-test.</p> </li> <li> <code>times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the data was sampled.</p> </li> <li> <code>p_threshold</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The p-value threshold to use. The default is 0.05.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>threshold_times</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The times to plot the p-value threshold at. If None, this is set to the times at which the data was sampled. Useful when using this plot underneath a time series plot.</p> </li> </ul> Source code in <code>red/plot.py</code> <pre><code>def plot_p_values(\n    ax: _Axes,\n    p_values: _npt.NDArray[_np.float64],\n    times: _npt.NDArray[_np.float64],\n    p_threshold: float = 0.05,\n    time_units: str = \"ns\",\n    threshold_times: _Optional[_npt.NDArray[_np.float64]] = None,\n) -&gt; None:\n    \"\"\"\n    Plot the p-values of the paired t-test.\n\n    Parameters\n    ----------\n    ax : Axes\n        The axes to plot on.\n\n    p_values : np.ndarray\n        The p-values of the paired t-test.\n\n    times : np.ndarray\n        The times at which the data was sampled.\n\n    p_threshold : float, optional\n        The p-value threshold to use. The default is 0.05.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    threshold_times : np.ndarray, optional\n        The times to plot the p-value threshold at. If None, this is\n        set to the times at which the data was sampled. Useful when\n        using this plot underneath a time series plot.\n    \"\"\"\n    # Check that p_values is valid.\n    if not isinstance(p_values, _np.ndarray) or not isinstance(times, _np.ndarray):\n        raise InvalidInputError(\"p_values and times must be numpy arrays.\")\n\n    if p_values.ndim != 1:\n        raise InvalidInputError(\"p_values must be one dimensional.\")\n\n    if p_values.shape[0] != times.shape[0]:\n        raise InvalidInputError(\"p_values must have the same length as the number of samples.\")\n\n    if threshold_times is None:\n        threshold_times = times\n\n    # Plot the p-values.\n    ax.scatter(times, p_values, color=\"black\", label=\"p-value\")\n\n    # Plot the p-value threshold.\n    ax.plot(\n        threshold_times,\n        _np.full(threshold_times.shape, p_threshold),\n        color=\"black\",\n        linestyle=\"-\",\n        linewidth=0.5,\n        label=\"p-value threshold\",\n    )\n\n    # Shade the region where the p-value is less than the threshold.\n    ax.fill_between(\n        threshold_times,\n        0,\n        p_threshold,\n        alpha=0.3,\n        # Black\n        color=\"black\",\n    )\n\n    # Plot a vertical dashed line at the first time point where the p-value is\n    # greater than the threshold.\n    ax.axvline(\n        x=times[p_values &gt; p_threshold][0],\n        color=\"black\",\n        linestyle=\"--\",\n        label=f\"Equilibration Time = {times[p_values &gt; p_threshold][0]:.3g} {time_units}\",\n    )\n\n    ax.legend()\n\n    # Set the axis labels.\n    ax.set_xlabel(f\"Time / {time_units}\")\n    ax.set_ylabel(\"$p$-value\")\n</code></pre>"},{"location":"reference/plot/#red.plot.plot_sse","title":"plot_sse","text":"<pre><code>plot_sse(\n    ax: Axes,\n    sse: NDArray[float64],\n    max_lags: Optional[NDArray[float64]],\n    window_sizes: Optional[NDArray[float64]],\n    times: NDArray[float64],\n    time_units: str = \"ns\",\n    variance_y_label: str = \"$\\\\frac{1}{\\\\sigma^2(\\\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n    reciprocal: bool = True,\n) -&gt; Tuple[List[Artist], List[Any]]\n</code></pre> <p>Plot the squared standard error (SSE) estimate against time.</p> <p>Parameters:</p> <ul> <li> <code>ax</code>               (<code>Axes</code>)           \u2013            <p>The axes to plot on.</p> </li> <li> <code>sse</code>               (<code>ndarray</code>)           \u2013            <p>The SSE estimate.</p> </li> <li> <code>max_lags</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The maximum lag times.</p> </li> <li> <code>window_sizes</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The window sizes.</p> </li> <li> <code>times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the data was sampled.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>variance_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\frac{1}{\\\\sigma^2(\\\\Delta G)}$ / kcal$^{-2}$ mol$^2$'</code> )           \u2013            <p>The y-axis label for the variance. The default is \"\\(\\frac{1}{\\sigma^2(\\Delta G)}\\) / kcal\\(^{-2}\\) mol\\(^2\\)\".</p> </li> <li> <code>reciprocal</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the reciprocal of the SSE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>handles</code> (              <code>List[Line2D]</code> )          \u2013            <p>The handles for the legend.</p> </li> <li> <code>labels</code> (              <code>List[str]</code> )          \u2013            <p>The labels for the legend.</p> </li> </ul> Source code in <code>red/plot.py</code> <pre><code>def plot_sse(\n    ax: _Axes,\n    sse: _npt.NDArray[_np.float64],\n    max_lags: _Optional[_npt.NDArray[_np.float64]],\n    window_sizes: _Optional[_npt.NDArray[_np.float64]],\n    times: _npt.NDArray[_np.float64],\n    time_units: str = \"ns\",\n    variance_y_label: str = r\"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n    reciprocal: bool = True,\n) -&gt; _Tuple[_List[_Artist], _List[_Any]]:\n    r\"\"\"\n    Plot the squared standard error (SSE) estimate against time.\n\n    Parameters\n    ----------\n    ax : Axes\n        The axes to plot on.\n\n    sse : np.ndarray\n        The SSE estimate.\n\n    max_lags : np.ndarray, optional, default=None\n        The maximum lag times.\n\n    window_sizes : np.ndarray, optional, default=None\n        The window sizes.\n\n    times : np.ndarray\n        The times at which the data was sampled.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    variance_y_label : str, optional\n        The y-axis label for the variance. The default is\n        \"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\".\n\n    reciprocal : bool, optional, default=True\n        Whether to plot the reciprocal of the SSE.\n\n    Returns\n    -------\n    handles : List[Line2D]\n        The handles for the legend.\n\n    labels : List[str]\n        The labels for the legend.\n    \"\"\"\n    # Check that sse is valid.\n    if not isinstance(sse, _np.ndarray) or not isinstance(times, _np.ndarray):\n        raise InvalidInputError(\"sse and times must be numpy arrays.\")\n\n    if sse.ndim != 1:\n        raise InvalidInputError(\"sse must be one dimensional.\")\n\n    if sse.shape[0] != times.shape[0]:\n        raise InvalidInputError(\"sse must have the same length as the number of samples.\")\n\n    if max_lags is not None and window_sizes is not None:\n        raise InvalidInputError(\"Only one of max_lags and window_sizes can be supplied.\")\n\n    with _plt.style.context(PLT_STYLE):\n        # Plot the SSE.\n        to_plot = 1 / sse if reciprocal else sse\n        label = \"1/SSE\" if reciprocal else \"SSE\"\n        if \"ess\" in variance_y_label.lower():\n            label = \"ESS\" if reciprocal else \"1/ESS\"\n        ax.plot(times, to_plot, color=\"black\", label=label)\n\n        # If lags is not None, plot the lag times on a different y axis.\n        if max_lags is not None or window_sizes is not None:\n            label = \"Max Lag Index\" if window_sizes is None else \"Window Size\"\n            to_plot = max_lags if window_sizes is None else window_sizes  # type: ignore\n            ax2 = ax.twinx()\n            # Get the second colour from the colour cycle.\n            ax2.set_prop_cycle(color=[_plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][1]])\n            ax2.plot(times, to_plot, alpha=0.8, label=label)\n            # Remove the horizontal lines.\n            ax2.yaxis.grid(False)\n\n            # Set the grid ticks and label colour to match the line colour.\n            lag_colour = ax2.get_lines()[0].get_color()\n            ax2.tick_params(axis=\"y\", labelcolor=lag_colour)\n            ax2.set_ylabel(label, color=lag_colour)\n\n        # Plot a vertical dashed line at the minimum SSE.\n        ax.axvline(\n            x=times[sse.argmin()],\n            color=\"black\",\n            linestyle=\"--\",\n            label=f\"Equilibration Time = {times[sse.argmin()]:.3g} {time_units}\",\n        )\n\n        # Combine the legends from both axes.\n        handles, labels = ax.get_legend_handles_labels()\n        if max_lags is not None or window_sizes is not None:\n            handles2, labels2 = ax2.get_legend_handles_labels()\n            handles += handles2\n            labels += labels2\n\n        ax.legend(handles, labels)\n\n        # Set the axis labels.\n        ax.set_xlabel(f\"Time / {time_units}\")\n        ax.set_ylabel(variance_y_label)\n\n        return handles, labels\n</code></pre>"},{"location":"reference/plot/#red.plot.plot_equilibration_paired_t_test","title":"plot_equilibration_paired_t_test","text":"<pre><code>plot_equilibration_paired_t_test(\n    fig: Figure,\n    subplot_spec: SubplotSpec,\n    data: NDArray[float64],\n    p_values: NDArray[float64],\n    data_times: NDArray[float64],\n    p_times: NDArray[float64],\n    p_threshold: float = 0.05,\n    time_units: str = \"ns\",\n    data_y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; Tuple[Axes, Axes]\n</code></pre> <p>Plot the p-values of the paired t-test against time, underneath the time series data.</p> <p>Parameters:</p> <ul> <li> <code>fig</code>               (<code>Figure</code>)           \u2013            <p>The figure to plot on.</p> </li> <li> <code>gridspec_obj</code>               (<code>GridSpec</code>)           \u2013            <p>The gridspec to use for the plot.</p> </li> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples), or (n_samples,) if there is only one run.</p> </li> <li> <code>p_values</code>               (<code>ndarray</code>)           \u2013            <p>The p-values of the paired t-test.</p> </li> <li> <code>data_times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the data was sampled.</p> </li> <li> <code>p_times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the paired t-test was performed.</p> </li> <li> <code>p_threshold</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The p-value threshold to use. The default is 0.05.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>data_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label for the time series data. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ax_top</code> (              <code>Axes</code> )          \u2013            <p>The axes for the time series data.</p> </li> <li> <code>ax_bottom</code> (              <code>Axes</code> )          \u2013            <p>The axes for the p-values.</p> </li> </ul> Source code in <code>red/plot.py</code> <pre><code>def plot_equilibration_paired_t_test(\n    fig: _figure.Figure,\n    subplot_spec: _gridspec.SubplotSpec,\n    data: _npt.NDArray[_np.float64],\n    p_values: _npt.NDArray[_np.float64],\n    data_times: _npt.NDArray[_np.float64],\n    p_times: _npt.NDArray[_np.float64],\n    p_threshold: float = 0.05,\n    time_units: str = \"ns\",\n    data_y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n) -&gt; _Tuple[_Axes, _Axes]:\n    r\"\"\"\n    Plot the p-values of the paired t-test against time, underneath the\n    time series data.\n\n    Parameters\n    ----------\n    fig : plt.Figure\n        The figure to plot on.\n\n    gridspec_obj : plt.GridSpec\n        The gridspec to use for the plot.\n\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples), or (n_samples,) if there\n        is only one run.\n\n    p_values : np.ndarray\n        The p-values of the paired t-test.\n\n    data_times : np.ndarray\n        The times at which the data was sampled.\n\n    p_times : np.ndarray\n        The times at which the paired t-test was performed.\n\n    p_threshold : float, optional\n        The p-value threshold to use. The default is 0.05.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    data_y_label : str, optional\n        The y-axis label for the time series data. The default is\n        \"$\\Delta G$ / kcal mol$^{-1}$\".\n\n    Returns\n    -------\n    ax_top : Axes\n        The axes for the time series data.\n\n    ax_bottom : Axes\n        The axes for the p-values.\n    \"\"\"\n    with _plt.style.context(PLT_STYLE):\n        # We need to split the gridspec into two subplots, one for the time series data (above)\n        # and one for the p-values (below). Share x-axis but not y-axis.\n        gs0 = _gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=subplot_spec, hspace=0.05)\n        ax_top = fig.add_subplot(gs0[0])\n        ax_bottom = fig.add_subplot(gs0[1], sharex=ax_top)\n\n        # Plot the time series data on the top axis.\n        plot_timeseries(ax_top, data, data_times)\n        # Add dashed vertical line at the equilibration time.\n        ax_top.axvline(\n            x=p_times[p_values &gt; p_threshold][0],\n            color=\"black\",\n            linestyle=\"--\",\n        )\n\n        # Plot the p-values on the bottom axis.\n        plot_p_values(\n            ax_bottom,\n            p_values,\n            p_times,\n            p_threshold=p_threshold,\n            threshold_times=data_times,\n        )\n\n        # Set the axis labels.\n        ax_top.set_xlabel(f\"Time / {time_units}\")\n        ax_top.set_ylabel(data_y_label)\n\n        # Move the legends to the side of the plot.\n        ax_top.legend(bbox_to_anchor=(1.05, 0.5), loc=\"center left\")\n        ax_bottom.legend(bbox_to_anchor=(1.05, 0.5), loc=\"center left\")\n\n        # Hide the x tick labels for the top axis.\n        _plt.setp(ax_top.get_xticklabels(), visible=False)\n        ax_top.spines[\"bottom\"].set_visible(False)\n        ax_top.tick_params(axis=\"x\", which=\"both\", length=0)\n        ax_top.set_xlabel(\"\")\n\n        return ax_top, ax_bottom\n</code></pre>"},{"location":"reference/plot/#red.plot.plot_equilibration_min_sse","title":"plot_equilibration_min_sse","text":"<pre><code>plot_equilibration_min_sse(\n    fig: Figure,\n    subplot_spec: SubplotSpec,\n    data: NDArray[float64],\n    sse_series: NDArray[float64],\n    data_times: NDArray[float64],\n    sse_times: NDArray[float64],\n    max_lag_series: Optional[NDArray[float64]] = None,\n    window_size_series: Optional[NDArray[float64]] = None,\n    time_units: str = \"ns\",\n    data_y_label: str = \"$\\\\Delta G$ / kcal mol$^{-1}$\",\n    variance_y_label: str = \"$\\\\frac{1}{\\\\sigma^2(\\\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n    reciprocal: bool = True,\n) -&gt; Tuple[Axes, Axes]\n</code></pre> <p>Plot the (reciprocal of the) squared standard error (SSE) estimates against time, underneath the time series data.</p> <p>Parameters:</p> <ul> <li> <code>fig</code>               (<code>Figure</code>)           \u2013            <p>The figure to plot on.</p> </li> <li> <code>subplot_spec</code>               (<code>GridSpec</code>)           \u2013            <p>The gridspec to use for the plot.</p> </li> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_runs, n_samples), or (n_samples,) if there is only one run.</p> </li> <li> <code>sse_series</code>               (<code>ndarray</code>)           \u2013            <p>The SSE series.</p> </li> <li> <code>data_times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the data was sampled.</p> </li> <li> <code>sse_times</code>               (<code>ndarray</code>)           \u2013            <p>The times at which the ESS was computed.</p> </li> <li> <code>max_lag_series</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The lag series. If None, the lag times are not plotted. If supplied, they are plotted on the bottom axis.</p> </li> <li> <code>window_size_series</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The window size series. If None, the window sizes are not plotted. If supplied, they are plotted on the bottom axis.</p> </li> <li> <code>time_units</code>               (<code>str</code>, default:                   <code>'ns'</code> )           \u2013            <p>The units of time. The default is \"ns\".</p> </li> <li> <code>data_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\Delta G$ / kcal mol$^{-1}$'</code> )           \u2013            <p>The y-axis label for the time series data. The default is \"\\(\\Delta G\\) / kcal mol\\(^{-1}\\)\".</p> </li> <li> <code>variance_y_label</code>               (<code>str</code>, default:                   <code>'$\\\\frac{1}{\\\\sigma^2(\\\\Delta G)}$ / kcal$^{-2}$ mol$^2$'</code> )           \u2013            <p>The y-axis label for the variance. The default is \"\\(\\frac{1}{\\sigma^2(\\Delta G)}\\) / kcal\\(^{-2}\\) mol\\(^2\\)\".</p> </li> <li> <code>reciprocal</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to plot the reciprocal of the SSE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ax_top</code> (              <code>Axes</code> )          \u2013            <p>The axes for the time series data.</p> </li> <li> <code>ax_bottom</code> (              <code>Axes</code> )          \u2013            <p>The axes for the p-values.</p> </li> </ul> Source code in <code>red/plot.py</code> <pre><code>def plot_equilibration_min_sse(\n    fig: _figure.Figure,\n    subplot_spec: _gridspec.SubplotSpec,\n    data: _npt.NDArray[_np.float64],\n    sse_series: _npt.NDArray[_np.float64],\n    data_times: _npt.NDArray[_np.float64],\n    sse_times: _npt.NDArray[_np.float64],\n    max_lag_series: _Optional[_npt.NDArray[_np.float64]] = None,\n    window_size_series: _Optional[_npt.NDArray[_np.float64]] = None,\n    time_units: str = \"ns\",\n    data_y_label: str = r\"$\\Delta G$ / kcal mol$^{-1}$\",\n    variance_y_label: str = r\"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\",\n    reciprocal: bool = True,\n) -&gt; _Tuple[_Axes, _Axes]:\n    r\"\"\"\n    Plot the (reciprocal of the) squared standard error (SSE)\n    estimates against time, underneath the time series data.\n\n    Parameters\n    ----------\n    fig : plt.Figure\n        The figure to plot on.\n\n    subplot_spec : plt.GridSpec\n        The gridspec to use for the plot.\n\n    data : np.ndarray\n        The time series data. This should have shape\n        (n_runs, n_samples), or (n_samples,) if there\n        is only one run.\n\n    sse_series : np.ndarray\n        The SSE series.\n\n    data_times : np.ndarray\n        The times at which the data was sampled.\n\n    sse_times : np.ndarray\n        The times at which the ESS was computed.\n\n    max_lag_series : np.ndarray, optional\n        The lag series. If None, the lag times are not\n        plotted. If supplied, they are plotted on the\n        bottom axis.\n\n    window_size_series : np.ndarray, optional\n        The window size series. If None, the window sizes\n        are not plotted. If supplied, they are plotted on\n        the bottom axis.\n\n    time_units : str, optional\n        The units of time. The default is \"ns\".\n\n    data_y_label : str, optional\n        The y-axis label for the time series data. The default is\n        \"$\\Delta G$ / kcal mol$^{-1}$\".\n\n    variance_y_label : str, optional\n        The y-axis label for the variance. The default is\n        \"$\\frac{1}{\\sigma^2(\\Delta G)}$ / kcal$^{-2}$ mol$^2$\".\n\n    reciprocal : bool, optional, default=True\n        Whether to plot the reciprocal of the SSE.\n\n    Returns\n    -------\n    ax_top : Axes\n        The axes for the time series data.\n\n    ax_bottom : Axes\n        The axes for the p-values.\n    \"\"\"\n    with _plt.style.context(PLT_STYLE):\n        data = check_data(data, one_dim_allowed=True)\n        n_runs, _ = data.shape\n\n        # We need to split the gridspec into two subplots, one for the time series data (above)\n        # and one for the p-values (below). Share x-axis but not y-axis.\n        gs0 = _gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=subplot_spec, hspace=0.05)\n        ax_top = fig.add_subplot(gs0[0])\n        ax_bottom = fig.add_subplot(gs0[1], sharex=ax_top)\n\n        # Plot the time series data on the top axis.\n        plot_timeseries(ax_top, data, data_times)\n        # Add dashed vertical line at the equilibration time.\n        ax_top.axvline(\n            x=sse_times[sse_series.argmin()],\n            color=\"black\",\n            linestyle=\"--\",\n        )\n\n        # Plot the sse on the bottom axis.\n        sse_handles, sse_labels = plot_sse(\n            ax_bottom,\n            sse_series,\n            max_lag_series,\n            window_size_series,\n            sse_times,\n            variance_y_label=variance_y_label,\n            reciprocal=reciprocal,\n        )\n\n        # Set the axis labels.\n        ax_top.set_xlabel(f\"Time / {time_units}\")\n        ax_top.set_ylabel(data_y_label)\n\n        # Move the legends to the side of the plot.\n        if n_runs &gt; 1:\n            ax_top.legend(bbox_to_anchor=(1.05, 0.5), loc=\"center left\")\n        is_second_axis = max_lag_series is not None or window_size_series is not None\n        side_shift_bottom = 1.15 if is_second_axis else 1.05\n        # Remove the 1/SSE label if there isn't a second axis.\n        if not is_second_axis:\n            sse_labels.pop(0)\n            sse_handles.pop(0)\n        ax_bottom.legend(\n            sse_handles,\n            sse_labels,\n            bbox_to_anchor=(side_shift_bottom, 0.5),\n            loc=\"center left\",\n        )\n\n        # Hide the x tick labels for the top axis.\n        _plt.setp(ax_top.get_xticklabels(), visible=False)\n        ax_top.spines[\"bottom\"].set_visible(False)\n        ax_top.tick_params(axis=\"x\", which=\"both\", length=0)\n        ax_top.set_xlabel(\"\")\n\n        return ax_top, ax_bottom\n</code></pre>"},{"location":"reference/sse/","title":"sse","text":""},{"location":"reference/sse/#red.sse","title":"sse","text":"<p>Functions to calculate the squared standard error series.</p>"},{"location":"reference/sse/#red.sse.get_sse_series_init_seq","title":"get_sse_series_init_seq","text":"<pre><code>get_sse_series_init_seq(\n    data: NDArray[float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Compute a series of squared standard errors for a time series as data is discarded from the beginning of the time series. The squared standard error is computed using the sequence estimator specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> <li> <code>smooth_lag_times</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to smooth out the max lag times by a) converting them to a monotinically decreasing sequence and b) linearly interpolating between points where the sequence changes. This may be useful when the max lag times are noisy.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The squared standard error series.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The maximum lag times used.</p> </li> </ul> Source code in <code>red/sse.py</code> <pre><code>def get_sse_series_init_seq(\n    data: _npt.NDArray[_np.float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Compute a series of squared standard errors for a time series as data\n    is discarded from the beginning of the time series. The squared standard\n    error is computed using the sequence estimator specified.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    smooth_lag_times : bool, optional, default=False\n        Whether to smooth out the max lag times by a) converting them to a monotinically\n        decreasing sequence and b) linearly interpolating between points where the sequence\n        changes. This may be useful when the max lag times are noisy.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    Returns\n    -------\n    np.ndarray\n        The squared standard error series.\n\n    np.ndarray\n        The maximum lag times used.\n    \"\"\"\n    # Validate the data.\n    data = check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n\n    # Compute the variance estimates.\n    var_series, max_lag_times = get_variance_series_initial_sequence(\n        data,\n        sequence_estimator=sequence_estimator,\n        min_max_lag_time=min_max_lag_time,\n        max_max_lag_time=max_max_lag_time,\n        smooth_lag_times=smooth_lag_times,\n        frac_padding=frac_padding,\n    )\n\n    # Compute the squared standard error series by dividing the variance series by\n    # the total number of samples.\n    tot_samples = _np.arange(n_samples, n_samples - len(var_series), -1) * n_runs\n    sse_series = var_series / tot_samples\n\n    return sse_series, max_lag_times\n</code></pre>"},{"location":"reference/sse/#red.sse.get_sse_series_window","title":"get_sse_series_window","text":"<pre><code>get_sse_series_window(\n    data: NDArray[float64],\n    kernel: Callable[\n        [int], NDArray[float64]\n    ] = _np.bartlett,\n    window_size_fn: Optional[\n        Callable[[int], int]\n    ] = lambda x: round(x**0.5),\n    window_size: Optional[int] = None,\n    frac_padding: float = 0.1,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Compute a series of squared standard errors for a time series as data is discarded from the beginning of the time series. The squared standard error is computed using the window size and kernel specified.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>kernel</code>               (<code>callable</code>, default:                   <code>numpy.bartlett</code> )           \u2013            <p>A function that takes a window size and returns a window function.</p> </li> <li> <code>window_size_fn</code>               (<code>callable</code>, default:                   <code>lambda x: round(x**0.5)</code> )           \u2013            <p>A function that takes the length of the time series and returns the window size to use. If this is not None, window_size must be None.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The size of the window to use, defined in terms of time lags in the forwards direction. If this is not None, window_size_fn must be None.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The squared standard error series.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The window sizes used.</p> </li> </ul> Source code in <code>red/sse.py</code> <pre><code>def get_sse_series_window(\n    data: _npt.NDArray[_np.float64],\n    kernel: _Callable[[int], _npt.NDArray[_np.float64]] = _np.bartlett,  # type: ignore\n    window_size_fn: _Optional[_Callable[[int], int]] = lambda x: round(x**0.5),\n    window_size: _Optional[int] = None,\n    frac_padding: float = 0.1,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Compute a series of squared standard errors for a time series as data\n    is discarded from the beginning of the time series. The squared standard\n    error is computed using the window size and kernel specified.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    kernel : callable, optional, default=numpy.bartlett\n        A function that takes a window size and returns a window function.\n\n    window_size_fn : callable, optional, default=lambda x: round(x**0.5)\n        A function that takes the length of the time series and returns the window size\n        to use. If this is not None, window_size must be None.\n\n    window_size : int, optional, default=None\n        The size of the window to use, defined in terms of time lags in the\n        forwards direction. If this is not None, window_size_fn must be None.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    Returns\n    -------\n    np.ndarray\n        The squared standard error series.\n\n    np.ndarray\n        The window sizes used.\n    \"\"\"\n    # Validate the data.\n    data = check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n\n    # Compute the variance estimates.\n    var_series, window_sizes = get_variance_series_window(\n        data,\n        kernel=kernel,\n        window_size_fn=window_size_fn,\n        window_size=window_size,\n        frac_padding=frac_padding,\n    )\n\n    # Compute the squared standard error series by dividing the variance series by\n    # the total number of samples.\n    tot_samples = _np.arange(n_samples, n_samples - len(var_series), -1) * n_runs\n    sse_series = var_series / tot_samples\n\n    return sse_series, window_sizes\n</code></pre>"},{"location":"reference/variance/","title":"variance","text":""},{"location":"reference/variance/#red.variance","title":"variance","text":"<p>Functions to calculate the variance of a time series, accounting for autocorrelation.</p> <p>Methods implemented:</p> <ul> <li>Initial sequence methods (see Geyer, 1992: https://www.jstor.org/stable/2246094)</li> <li>Window estimators (see summary in see Geyer, 1992: https://www.jstor.org/stable/2246094)</li> </ul> <p>Did not implement overlapping batch means (see Meketon and Schmeiser, 1984: https://repository.lib.ncsu.edu/bitstream/handle/1840.4/7707/1984_0041.pdf?sequence=1), as this is equivalent to using a Bartlett window.</p>"},{"location":"reference/variance/#red.variance.get_variance_initial_sequence","title":"get_variance_initial_sequence","text":"<pre><code>get_variance_initial_sequence(\n    data: NDArray[float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n    autocov: Optional[NDArray[float64]] = None,\n) -&gt; Tuple[float, int, NDArray[float64]]\n</code></pre> <p>Calculate the variance of a time series using initial sequence methods. See Geyer, 1992: https://www.jstor.org/stable/2246094.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> <li> <code>autocov</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>The auto-covariance function of the time series. If None, this will be calculated from the time series.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The estimated variance of the time series, accounting for correlation.</p> </li> <li> <code>int</code>           \u2013            <p>The maximum lag time used when calculating the auto-correlated variance.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The auto-covariance function of the time series.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def get_variance_initial_sequence(\n    data: _npt.NDArray[_np.float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n    autocov: _Optional[_npt.NDArray[_np.float64]] = None,\n) -&gt; _Tuple[float, int, _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Calculate the variance of a time series using initial sequence methods.\n    See Geyer, 1992: https://www.jstor.org/stable/2246094.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    autocov : numpy.ndarray, optional, default=None\n        The auto-covariance function of the time series. If None, this will be calculated\n        from the time series.\n\n    Returns\n    -------\n    float\n        The estimated variance of the time series, accounting for correlation.\n\n    int\n        The maximum lag time used when calculating the auto-correlated variance.\n\n    numpy.ndarray\n        The auto-covariance function of the time series.\n    \"\"\"\n    # Validate the data.\n    data = _check_data(data, one_dim_allowed=True)\n    n_runs, n_samples = data.shape\n\n    # Check that sequence_estimator is valid.\n    implemented_sequence_estimators = [\n        \"positive\",\n        \"initial_positive\",\n        \"initial_monotone\",\n        \"initial_convex\",\n    ]\n    if sequence_estimator not in implemented_sequence_estimators:\n        raise InvalidInputError(\n            f\"Sequence_estimator must be one of {implemented_sequence_estimators}.\"\n        )\n\n    # Check that the minimum maximum lag time is valid.\n    if min_max_lag_time &lt; 0:\n        raise InvalidInputError(\"Minimum maximum lag time must be greater than or equal to 0.\")\n\n    if min_max_lag_time &gt; n_samples - 1:\n        raise InvalidInputError(\n            \"Minimum maximum lag time must be less than or equal to the number of samples minus 1.\"\n        )\n\n    # Make sure that the maximum lag time is valid.\n    if max_max_lag_time is not None:\n        if max_max_lag_time &lt; 0:\n            raise InvalidInputError(\"Maximum lag time must be greater than or equal to 0.\")\n\n        if max_max_lag_time &gt; n_samples - 1:\n            raise InvalidInputError(\n                \"Maximum lag time must be less than or equal to the number of samples minus 1. \"\n                f\"Maximum lag time is {max_max_lag_time} and number of samples is {n_samples}.\"\n            )\n\n        if max_max_lag_time &lt; min_max_lag_time:\n            raise InvalidInputError(\n                \"Maximum lag time must be greater than or equal to the minimum maximum lag time.\"\n            )\n\n        if autocov is not None:\n            if max_max_lag_time &gt; autocov.shape[0] - 1:\n                raise InvalidInputError(\n                    \"Maximum lag time must be less than or equal to the length of the\"\n                    \"autocovariance function minus 1.\"\n                )\n\n    # Check that autocov_series is valid.\n    if autocov is not None:\n        if not isinstance(autocov, _np.ndarray):\n            raise InvalidInputError(\"Autocovariance must be a numpy.ndarray.\")\n\n        if autocov.ndim != 1:\n            raise InvalidInputError(\"Autocovariance must be one-dimensional.\")\n\n        if autocov.shape[0] &lt; 2:\n            raise InvalidInputError(\"Autocovariance must have at least two elements.\")\n\n    # Get the uncorrected variance estimate.\n    var = data.var()\n    if var == 0:\n        raise AnalysisError(\n            \"Variance of data is zero. Cannot compute variance. \"\n            \"Check that you have input the correct data.\"\n        )\n\n    if autocov is None:\n        # Get the mean autocovariance as a function of lag time across all runs,\n        # using the shared mean. Use FFT as we are usually calculating the autocovariance\n        # for large arrays.\n        autocov_valid = _np.mean(\n            [\n                _get_autocovariance(\n                    data[run],\n                    mean=data.mean(),\n                    max_lag=max_max_lag_time,\n                    fft=True,\n                )\n                for run in range(n_runs)\n            ],\n            axis=0,\n        )\n    else:\n        # Create autocov_valid to satisfy the type checker.\n        autocov_valid = autocov\n\n    # If using the positive estimator, truncate the autocovariance at the\n    # first negative value, if this exists.\n    if sequence_estimator == \"positive\":\n        sub_zero_idxs = _np.where(autocov_valid &lt; 0)[0]\n        truncate_idx = sub_zero_idxs[0] if sub_zero_idxs.size &gt; 0 else len(autocov_valid)\n        # Limit the truncate in\n        autocov_valid = autocov_valid[:truncate_idx]\n        var_cor = autocov_valid.sum() * 2 - var\n        # Ensure that the variance can't be less than the uncorrelated value.\n        var_cor = max(var_cor, var)\n        max_lag_time_used = truncate_idx - 1\n        return var_cor, max_lag_time_used, autocov_valid\n\n    # Otherwise, get the gamma function. Avoid recalculating if\n    # it has already been provided.\n    gamma_cap = _get_gamma_cap(autocov_valid)\n    variance_fns = {\n        \"initial_positive\": _get_initial_positive_sequence,\n        \"initial_monotone\": _get_initial_monotone_sequence,\n        \"initial_convex\": _get_initial_convex_sequence,\n    }\n    gamma_cap = variance_fns[sequence_estimator](gamma_cap, min_max_lag_time=min_max_lag_time)\n    var_cor = gamma_cap.sum() * 2 - var\n\n    # Make sure that the variance is not negative.\n    var_cor = max(var_cor, var)\n\n    # Get the maximum lag time.\n    max_lag_time_used = gamma_cap.shape[0] * 2 - 1\n\n    return var_cor, max_lag_time_used, autocov_valid\n</code></pre>"},{"location":"reference/variance/#red.variance.get_variance_series_initial_sequence","title":"get_variance_series_initial_sequence","text":"<pre><code>get_variance_series_initial_sequence(\n    data: NDArray[float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Repeatedly calculate the variance of a time series while discarding increasing numbers of samples from the start of the time series. The variance is calculated using initial sequence methods. See Geyer, 1992: https://www.jstor.org/stable/2246094.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>sequence_estimator</code>               (<code>str</code>, default:                   <code>'initial_convex'</code> )           \u2013            <p>The initial sequence estimator to use. Can be \"positive\", \"initial_positive\", \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\" corresponds to truncating the auto-covariance function at the first negative value, as is done in pymbar. The other methods correspond to the methods described in Geyer, 1992: https://www.jstor.org/stable/2246094.</p> </li> <li> <code>min_max_lag_time</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The minimum maximum lag time to use when estimating the statistical inefficiency.</p> </li> <li> <code>max_max_lag_time</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum maximum lag time to use when calculating the auto-correlation function. If None, the maximum lag time will be the length of the time series.</p> </li> <li> <code>smooth_lag_times</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to smooth out the max lag times by a) converting them to a monotinically decreasing sequence and b) linearly interpolating between points where the sequence changes. This may be useful when the max lag times are noisy.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The variance of the time series as a function of the number of discarded samples.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The maximum lag time used when calculating the auto-correlated variance.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def get_variance_series_initial_sequence(\n    data: _npt.NDArray[_np.float64],\n    sequence_estimator: str = \"initial_convex\",\n    min_max_lag_time: int = 3,\n    max_max_lag_time: _Optional[int] = None,\n    smooth_lag_times: bool = False,\n    frac_padding: float = 0.1,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Repeatedly calculate the variance of a time series while discarding increasing\n    numbers of samples from the start of the time series. The variance is calculated\n    using initial sequence methods. See Geyer, 1992: https://www.jstor.org/stable/2246094.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    sequence_estimator : str, optional\n        The initial sequence estimator to use. Can be \"positive\", \"initial_positive\",\n        \"initial_monotone\", or \"initial_convex\". The default is \"initial_convex\". \"positive\"\n        corresponds to truncating the auto-covariance function at the first negative value, as is\n        done in pymbar. The other methods correspond to the methods described in Geyer, 1992:\n        https://www.jstor.org/stable/2246094.\n\n    min_max_lag_time : int, optional, default=3\n        The minimum maximum lag time to use when estimating the statistical inefficiency.\n\n    max_max_lag_time : int, optional, default=None\n        The maximum maximum lag time to use when calculating the auto-correlation function.\n        If None, the maximum lag time will be the length of the time series.\n\n    smooth_lag_times : bool, optional, default=False\n        Whether to smooth out the max lag times by a) converting them to a monotinically\n        decreasing sequence and b) linearly interpolating between points where the sequence\n        changes. This may be useful when the max lag times are noisy.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    Returns\n    -------\n    numpy.ndarray\n        The variance of the time series as a function of the number of discarded samples.\n\n    numpy.ndarray\n        The maximum lag time used when calculating the auto-correlated variance.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=True)\n    n_samples = data.shape[1]\n\n    # Check that percent_padding is valid.\n    if frac_padding &lt; 0 or frac_padding &gt;= 1:\n        raise InvalidInputError(\"Percent padding must be &gt;= 0 and &lt; 1.\")\n\n    if frac_padding &gt; 0.5:\n        _warn(\n            \"Percent padding is greater than 0.5. You are evaluating less than half of the data.\",\n            stacklevel=2,\n        )\n\n    # Calculate the maximum index to use when discarding samples.\n    max_index = n_samples - 1 - min_max_lag_time if min_max_lag_time else n_samples - 2\n\n    # Needs to be a max of n_samples - 2 to allow the gamma function to be calculated.\n    max_index = min(max_index, n_samples - 2)\n\n    # See if we need to truncate the max index even further based on the percent padding.\n    if frac_padding &gt; 0:\n        frac_padding_max_index = round(n_samples * (1 - frac_padding))\n        max_index = min(max_index, frac_padding_max_index)\n\n    # Calculate the variance at each index, and also the maximum lag time used\n    # and the autocovariance function.\n    variance_series = _np.zeros(max_index + 1)\n    max_lag_times_used = _np.zeros(max_index + 1, dtype=int)\n    # A list of autocovariance functions.\n    autocov_series = []\n\n    for index in range(max_index + 1):\n        variance, max_lag_time_used, autocov = get_variance_initial_sequence(\n            data[:, index:],\n            sequence_estimator=sequence_estimator,\n            min_max_lag_time=min_max_lag_time,\n            max_max_lag_time=max_max_lag_time,\n        )\n        variance_series[index] = variance\n        max_lag_times_used[index] = max_lag_time_used\n        autocov_series.append(autocov)\n\n        # If we are smoothing the lags, set the max lag time to the\n        # maximum lag time used for the current index. This saves\n        # some time computing the full autocovariance function.\n        if smooth_lag_times:\n            max_max_lag_time = max_lag_time_used\n            # If it's the same length as the time series, subtract 1\n            # so that it works on the next iteration.\n            if max_max_lag_time == n_samples - index - 1:\n                max_max_lag_time -= 1\n\n    if smooth_lag_times:\n        # Get the smoothened max lag times.\n        max_lag_times_to_use_smooth = _smoothen_max_lag_times(max_lag_times_used)\n\n        # Truncate the autocovariance functions at the smoothened max lag times.\n        autocov_series = [\n            autocov[: max_lag_times_to_use_smooth[index] + 1]\n            for index, autocov in enumerate(autocov_series)\n        ]\n\n        # Recalculate the variance series.\n        variance_series = _np.zeros(max_index + 1)\n        max_lag_times_used = _np.zeros(max_index + 1, dtype=int)\n\n        for index in range(max_index + 1):\n            variance, max_lag_time_used, _ = get_variance_initial_sequence(\n                data[:, index:],\n                sequence_estimator=sequence_estimator,\n                min_max_lag_time=min_max_lag_time,\n                max_max_lag_time=max_lag_times_to_use_smooth[index],\n                autocov=autocov_series[index],\n            )\n            variance_series[index] = variance\n            max_lag_times_used[index] = max_lag_time_used\n\n    return variance_series, max_lag_times_used\n</code></pre>"},{"location":"reference/variance/#red.variance.get_variance_window","title":"get_variance_window","text":"<pre><code>get_variance_window(\n    data: NDArray[float64],\n    kernel: Callable[\n        [int], NDArray[float64]\n    ] = _np.bartlett,\n    window_size: int = 10,\n) -&gt; float\n</code></pre> <p>Calculate the variance of a time series using window estimators.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>kernel</code>               (<code>callable</code>, default:                   <code>numpy.bartlett</code> )           \u2013            <p>A function that takes a window size and returns a window function.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the window to use, defined in terms of time lags in the forwards direction.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The estimated variance of the time series.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def get_variance_window(\n    data: _npt.NDArray[_np.float64],\n    kernel: _Callable[[int], _npt.NDArray[_np.float64]] = _np.bartlett,  # type: ignore\n    window_size: int = 10,\n) -&gt; float:\n    \"\"\"\n    Calculate the variance of a time series using window estimators.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    kernel : callable, optional, default=numpy.bartlett\n        A function that takes a window size and returns a window function.\n\n    window_size : int, optional, default=10\n        The size of the window to use, defined in terms of time lags in the\n        forwards direction.\n\n    Returns\n    -------\n    float\n        The estimated variance of the time series.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=True)\n    n_samples = data.shape[1]\n\n    # Check that the window estimator is valid.\n    if not callable(kernel):\n        raise InvalidInputError(\"Window estimator must be a callable function.\")\n\n    # Check that the window size is valid.\n    if window_size &lt; 1:\n        raise InvalidInputError(\"Window size must be greater than or equal to 1.\")\n\n    if window_size &gt; n_samples - 1:\n        raise InvalidInputError(\n            \"Window size must be less than or equal to the number of samples minus 1.\"\n        )\n\n    # Get the uncorrected variance estimate.\n    var = data.var()\n    if var == 0:\n        raise AnalysisError(\n            \"Variance of data is zero. Cannot compute statistical inefficiency. \"\n            \"Check that you have input the correct data.\"\n        )\n\n    # Get the windowed autocovariance.\n    autocov = _get_autocovariance_window(data, kernel, window_size)\n\n    # Account for correlation in the forwards and backwards directions.\n    corr_var = autocov.sum() * 2 - var\n\n    # Make sure that the variance is not less than the uncorrelated value.\n    return max(corr_var, var)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/variance/#red.variance.get_variance_series_window","title":"get_variance_series_window","text":"<pre><code>get_variance_series_window(\n    data: NDArray[float64],\n    kernel: Callable[\n        [int], NDArray[float64]\n    ] = _np.bartlett,\n    window_size_fn: Optional[\n        Callable[[int], int]\n    ] = lambda x: round(x**0.5),\n    window_size: Optional[int] = None,\n    frac_padding: float = 0.1,\n) -&gt; Tuple[NDArray[float64], NDArray[float64]]\n</code></pre> <p>Repeatedly calculate the variance of a time series while discarding increasing numbers of samples from the start of the time series. The variance is calculated using window estimators.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>A time series of data with shape (n_samples,).</p> </li> <li> <code>kernel</code>               (<code>callable</code>, default:                   <code>numpy.bartlett</code> )           \u2013            <p>A function that takes a window size and returns a window function.</p> </li> <li> <code>window_size_fn</code>               (<code>callable</code>, default:                   <code>lambda x: round(x**0.5)</code> )           \u2013            <p>A function that takes the length of the time series and returns the window size to use. If this is not None, window_size must be None.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The size of the window to use, defined in terms of time lags in the forwards direction. If this is not None, window_size_fn must be None.</p> </li> <li> <code>frac_padding</code>               (<code>float</code>, default:                   <code>0.1</code> )           \u2013            <p>The fraction of the end of the timeseries to avoid calculating the variance for. For example, if frac_padding = 0.1, the variance will be calculated for the first 90% of the time series. This helps to avoid noise in the variance when there are few data points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>The variance of the time series as a function of the number of discarded samples.</p> </li> <li> <code>ndarray</code>           \u2013            <p>The window size used at each index.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def get_variance_series_window(\n    data: _npt.NDArray[_np.float64],\n    kernel: _Callable[[int], _npt.NDArray[_np.float64]] = _np.bartlett,  # type: ignore\n    window_size_fn: _Optional[_Callable[[int], int]] = lambda x: round(x**0.5),\n    window_size: _Optional[int] = None,\n    frac_padding: float = 0.1,\n) -&gt; _Tuple[_npt.NDArray[_np.float64], _npt.NDArray[_np.float64]]:\n    \"\"\"\n    Repeatedly calculate the variance of a time series while discarding increasing\n    numbers of samples from the start of the time series. The variance is calculated\n    using window estimators.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        A time series of data with shape (n_samples,).\n\n    kernel : callable, optional, default=numpy.bartlett\n        A function that takes a window size and returns a window function.\n\n    window_size_fn : callable, optional, default=lambda x: round(x**0.5)\n        A function that takes the length of the time series and returns the window size\n        to use. If this is not None, window_size must be None.\n\n    window_size : int, optional, default=None\n        The size of the window to use, defined in terms of time lags in the\n        forwards direction. If this is not None, window_size_fn must be None.\n\n    frac_padding : float, optional, default=0.1\n        The fraction of the end of the timeseries to avoid calculating the variance\n        for. For example, if frac_padding = 0.1, the variance will be calculated\n        for the first 90% of the time series. This helps to avoid noise in the\n        variance when there are few data points.\n\n    Returns\n    -------\n    numpy.ndarray\n        The variance of the time series as a function of the number of discarded samples.\n\n    numpy.ndarray\n        The window size used at each index.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=True)\n    n_samples = data.shape[1]\n\n    # Check that only one of window_size_fn and window_size is not None.\n    if window_size_fn is not None and window_size is not None:\n        raise InvalidInputError(\"Only one of window_size_fn and window_size can be not None.\")\n\n    if window_size_fn is None and window_size is None:\n        raise InvalidInputError(\"One of window_size_fn and window_size must be not None.\")\n\n    if window_size_fn is not None:\n        # Check that the window size function is valid.\n        if not callable(window_size_fn):\n            raise InvalidInputError(\"Window size function must be a callable function.\")\n\n    # Check that frac_padding is valid.\n    if frac_padding &lt; 0 or frac_padding &gt;= 1:\n        raise InvalidInputError(\"Percent padding must be &gt;= 0 and &lt; 1.\")\n\n    if frac_padding &gt; 0.5:\n        _warn(\n            \"Percent padding is greater than 0.5. You are evaluating less than half of the data.\",\n            stacklevel=2,\n        )\n\n    # Calculate the maximum index to use when discarding samples.\n    max_index = n_samples - 1 - window_size if window_size is not None else n_samples - 2\n\n    # See if we need to truncate the max index even further based on the percent padding.\n    if frac_padding &gt; 0:\n        frac_padding_max_index = round(n_samples * (1 - frac_padding))\n        max_index = min(max_index, frac_padding_max_index)\n\n    # Calculate the variance at each index and store the window size used.\n    variance_series = _np.zeros(max_index + 1)\n    window_size_series = _np.zeros(max_index + 1, dtype=int)\n\n    for index in range(max_index + 1):\n        window_size = window_size_fn(n_samples - index) if window_size_fn else window_size\n        variance_series[index] = get_variance_window(\n            data[:, index:],\n            kernel=kernel,\n            window_size=window_size,  # type: ignore\n        )\n        window_size_series[index] = window_size\n\n    return variance_series, window_size_series\n</code></pre>"},{"location":"reference/variance/#red.variance.replicated_batch_means_variance","title":"replicated_batch_means_variance","text":"<pre><code>replicated_batch_means_variance(\n    data: NDArray[float64], batch_size: int\n) -&gt; float\n</code></pre> <p>Estimate the variance of a time series using the replicated batch means method. See section 3.1 in Statist. Sci. 36(4): 518-529 (November 2021). DOI: 10.1214/20-STS812 .</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_chains, n_samples).</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The estimated variance.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def replicated_batch_means_variance(data: _npt.NDArray[_np.float64], batch_size: int) -&gt; float:\n    \"\"\"\n    Estimate the variance of a time series using the replicated batch means method.\n    See section 3.1 in Statist. Sci. 36(4): 518-529 (November 2021).\n    DOI: 10.1214/20-STS812 .\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape (n_chains, n_samples).\n\n    batch_size : int\n        The batch size to use.\n\n    Returns\n    -------\n    float\n        The estimated variance.\n    \"\"\"\n    data = _check_data(data, one_dim_allowed=True)\n\n    # Check that batch_size is valid.\n    n_chains, n_samples = data.shape\n    if batch_size &lt; 1 or batch_size &gt; n_samples:\n        raise InvalidInputError(\n            f\"batch_size must be between 1 and n_samples = {n_samples} (inclusive),\"\n            f\" but got {batch_size}.\"\n        )\n\n    # Compute the number of batches.\n    n_batches = n_samples // batch_size\n\n    # Compute the mean of each batch.\n    batch_means = _np.mean(\n        data[:, : n_batches * batch_size].reshape(n_chains, n_batches, batch_size),\n        axis=2,\n    )\n\n    # Compute the variance of the batch means.\n    batch_means_variance = _np.var(batch_means, ddof=1)\n\n    # Multiply by the batch size.\n    batch_means_variance *= batch_size\n\n    return batch_means_variance  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/variance/#red.variance.lugsail_variance","title":"lugsail_variance","text":"<pre><code>lugsail_variance(\n    data: NDArray[float64], n_pow: float = 1 / 3\n) -&gt; float\n</code></pre> <p>Estimate the variance of a time series using the lugsail method. See section 3.2 in Statist. Sci. 36(4): 518-529 (November 2021). DOI: 10.1214/20-STS812 .</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_chains, n_samples).</p> </li> <li> <code>n_pow</code>               (<code>float</code>, default:                   <code>1/3</code> )           \u2013            <p>The batch size is computed as floor(n_samples**n_pow). Recommended choices are 1/3 or 1/2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The estimated variance.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def lugsail_variance(data: _npt.NDArray[_np.float64], n_pow: float = 1 / 3) -&gt; float:\n    \"\"\"\n    Estimate the variance of a time series using the lugsail method.\n    See section 3.2 in Statist. Sci. 36(4): 518-529 (November 2021).\n    DOI: 10.1214/20-STS812 .\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape (n_chains, n_samples).\n\n    n_pow : float, optional, default=1/3\n        The batch size is computed as floor(n_samples**n_pow). Recommended\n        choices are 1/3 or 1/2.\n\n    Returns\n    -------\n    float\n        The estimated variance.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=True)\n\n    # Check that n_pow is valid.\n    if n_pow &lt;= 0 or n_pow &gt; 1:\n        raise InvalidInputError(f\"n_pow must be between 0 and 1 (inclusive), but got {n_pow}.\")\n\n    # Get the two batch sizes.\n    _, n_samples = data.shape\n    batch_size_large = int(_np.floor(n_samples**n_pow))  # type: ignore\n    batch_size_small = int(_np.floor(batch_size_large / 3))  # type: ignore\n\n    # Make sure that the batch sizes are valid.\n    if batch_size_large == batch_size_small or batch_size_small &lt; 1:\n        raise AnalysisError(\n            \"The batch sizes computed using n_pow are too small. Try a larger value of n_pow.\"\n        )\n\n    # Compute the variance of the batch means.\n    variance_large_batch = replicated_batch_means_variance(data, batch_size_large)\n    variance_small_batch = replicated_batch_means_variance(data, batch_size_small)\n\n    # Compute the lugsail variance.\n    lugsail_variance = 2 * variance_large_batch - variance_small_batch\n\n    return lugsail_variance\n</code></pre>"},{"location":"reference/variance/#red.variance.inter_run_variance","title":"inter_run_variance","text":"<pre><code>inter_run_variance(data: NDArray[float64]) -&gt; float\n</code></pre> <p>Compute the variance based on the inter-run differences between means.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_chains, n_samples).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The estimated variance.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def inter_run_variance(data: _npt.NDArray[_np.float64]) -&gt; float:\n    \"\"\"\n    Compute the variance based on the inter-run differences\n    between means.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape (n_chains, n_samples).\n\n    Returns\n    -------\n    float\n        The estimated variance.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=False)\n\n    # Compute the inter-run variance.\n    inter_run_variance = _np.var(_np.mean(data, axis=1), ddof=1)\n\n    # Multiply by the number of samples per run.\n    _, n_samples = data.shape\n    inter_run_variance *= n_samples\n\n    return inter_run_variance  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/variance/#red.variance.intra_run_variance","title":"intra_run_variance","text":"<pre><code>intra_run_variance(data: NDArray[float64]) -&gt; float\n</code></pre> <p>Compute the average intra-run variance estimate.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>ndarray</code>)           \u2013            <p>The time series data. This should have shape (n_chains, n_samples).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>The mean intra-run variance estimate.</p> </li> </ul> Source code in <code>red/variance.py</code> <pre><code>def intra_run_variance(data: _npt.NDArray[_np.float64]) -&gt; float:\n    \"\"\"\n    Compute the average intra-run variance estimate.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        The time series data. This should have shape (n_chains, n_samples).\n\n    Returns\n    -------\n    float\n        The mean intra-run variance estimate.\n    \"\"\"\n    # Check that the data is valid.\n    data = _check_data(data, one_dim_allowed=True)\n\n    # Compute the intra-run variance estimates.\n    intra_run_variance = _np.var(data, axis=1, ddof=1)\n\n    # Compute the mean intra-run variance estimate.\n    mean_intra_run_variance = _np.mean(intra_run_variance)\n\n    return mean_intra_run_variance  # type: ignore[no-any-return]\n</code></pre>"}]}